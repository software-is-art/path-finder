{
  "tasks": [
    {
      "id": 1,
      "title": "S-Expression Lexer and Parser Implementation",
      "description": "Implement the lexer and parser for PathFinder LISP's S-expression syntax, adhering to Scheme conventions and case-sensitive identifiers. This forms the foundational input processing layer.",
      "details": "Implement using recursive descent or a parser combinator library. Define tokens for '(', ')', symbols (case-sensitive), numbers (integers, potentially floats later), booleans (#t, #f), strings. The parser should transform a sequence of tokens into an Abstract Syntax Tree (AST) representing S-expressions (nested lists and atoms). Ensure proper handling of comments (e.g., using ';'). AST nodes should include types like `Atom(Symbol)`, `Atom(Number)`, `Atom(String)`, `Atom(Boolean)`, `SExpr(List[ASTNode])`.",
      "testStrategy": "Unit tests with a comprehensive suite of S-expressions: simple atoms, empty list, nested lists, lists with mixed atom types, comments, and edge cases (e.g., mismatched parentheses, invalid tokens). Validate the structure of the generated AST for each test case.",
      "priority": "medium",
      "dependencies": [
        "21"
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Token Types and AST Node Structures",
          "description": "Define the core data structures for tokens (e.g., LPAREN, RPAREN, SYMBOL, NUMBER, BOOLEAN, STRING, EOF) and Abstract Syntax Tree (AST) nodes (Atom(Symbol), Atom(Number), Atom(String), Atom(Boolean), SExpr(List[ASTNode])). This establishes the vocabulary for the lexer and parser.",
          "dependencies": [],
          "details": "Create enums or classes for token types, including their potential value payloads (e.g., string value for SYMBOL, numeric value for NUMBER). For AST nodes, define a base class/interface (e.g., `ASTNode`) and specific derived classes for each atom type and for S-expressions. `Atom(Symbol)` should store its string value preserving case. `SExpr` should hold a list of `ASTNode` children.",
          "status": "pending",
          "testStrategy": "No direct executable tests for definitions, but these structures will be validated implicitly by tests for lexer and parser. Optionally, create simple instantiation tests to ensure constructors and properties work as expected."
        },
        {
          "id": 2,
          "title": "Implement Lexer: Basic Tokenization (Parentheses, Symbols, Numbers, Booleans)",
          "description": "Implement the lexer to scan an input string and produce a sequence of tokens. This subtask focuses on recognizing parentheses `(` and `)`, case-sensitive symbols (identifiers), integers, and booleans (`#t`, `#f`). Handle whitespace (spaces, tabs, newlines) as delimiters, which should be ignored and not produce tokens.",
          "dependencies": [
            1
          ],
          "details": "The lexer function should take a string as input and return a list of Token objects. Implement using character-by-character scanning or regular expressions. For symbols, ensure they are captured case-sensitively. For numbers, initially support integers. Booleans `#t` and `#f` should be recognized as distinct keywords/tokens. Ensure proper handling of end-of-input (e.g., an EOF token).",
          "status": "pending",
          "testStrategy": "Unit test with various input strings: `(add 1 2)`, `#t`, `my-Symbol`, `(  symbol1   #f  123 )`. Verify the correct sequence, type, and value of tokens produced. Test edge cases like empty input, input with only whitespace, and input ending abruptly."
        },
        {
          "id": 3,
          "title": "Extend Lexer: Add String Literal and Comment Handling",
          "description": "Enhance the lexer to correctly tokenize string literals enclosed in double quotes (e.g., `\"hello world\"`), including support for basic escape characters (e.g., `\\\"` for a double quote within the string, `\\\\` for a backslash). Implement logic to identify and ignore single-line comments starting with `;` until the end of the line.",
          "dependencies": [
            2
          ],
          "details": "For string literals, parse content between double quotes. Implement support for `\\\"` and `\\\\` escape sequences. The string token's value should be the unescaped content. For comments, when a `;` is encountered, the lexer should skip all subsequent characters on that line. Comments should not produce any tokens.",
          "status": "pending",
          "testStrategy": "Unit test inputs with strings: `(\"a string\")`, `(\"with \\\"quotes\\\" and \\\\backslash\\\\\")`. Test inputs with comments: `(add 1 2) ; this is a comment`, `; entire line comment`, `(define x \"value\" ; comment after string\n  y 10)`. Verify correct token stream and that comments are ignored."
        },
        {
          "id": 4,
          "title": "Implement Parser: Atomic Expressions and Simple S-Expressions",
          "description": "Develop the initial parser logic to consume tokens produced by the lexer and build an AST. This subtask focuses on parsing atomic expressions (symbols, numbers, booleans, strings) into their corresponding `Atom` AST nodes, and parsing simple, non-nested S-expressions (e.g., `(a b c)`) into `SExpr` AST nodes containing a list of `Atom` nodes.",
          "dependencies": [
            1,
            3
          ],
          "details": "Implement using recursive descent or a parser combinator library. Create a main `parse` function that takes a list of tokens. A helper function, say `parse_expression`, will determine if the next token(s) form an atom or an S-expression. If it's an atom, convert the token to the appropriate `Atom` AST node. If it's an `LPAREN` token, start parsing an S-expression: consume subsequent tokens as atoms until an `RPAREN` is found. Handle errors for unexpected tokens or premature end of input (e.g., missing `RPAREN`).",
          "status": "pending",
          "testStrategy": "Unit test with token streams representing: `my-symbol`, `123`, `#t`, `\"a string\"`, `(one two three)`. Verify the generated AST structure is correct (e.g., `Atom(Symbol(\"my-symbol\"))`, `SExpr([Atom(Symbol(\"one\")), Atom(Symbol(\"two\")), Atom(Symbol(\"three\"))])`). Test error handling for mismatched parentheses at this simple level (e.g., `(a b` or `a b)`)."
        },
        {
          "id": 5,
          "title": "Extend Parser: Nested S-Expressions and Full AST Construction",
          "description": "Enhance the parser to handle arbitrarily nested S-expressions (e.g., `(a (b (c)) d)`). This involves making the S-expression parsing rule fully recursive, allowing S-expressions to contain other S-expressions as elements, in addition to atoms. Ensure the AST correctly represents the hierarchical structure.",
          "dependencies": [
            4
          ],
          "details": "Modify the S-expression parsing logic (e.g., within the part that handles `LPAREN`) to recursively call the `parse_expression` function for each element found before the closing `RPAREN`. This `parse_expression` function should now be capable of returning either an `Atom` node or an `SExpr` node. This recursive structure will naturally handle nesting. Ensure proper error handling for malformed nested structures.",
          "status": "pending",
          "testStrategy": "Unit test with token streams representing nested S-expressions: `(a (b c) d)`, `((()))`, `(list 1 (quote (a b)) \"str\")`, `()`. Verify the deeply nested AST structure is accurately constructed. Test error handling for complex mismatched parentheses or unexpected tokens within nested structures (e.g., `(a (b c d)`)."
        }
      ]
    },
    {
      "id": 2,
      "title": "Core AST Definition and Basic Evaluation Engine",
      "description": "Define the core Abstract Syntax Tree (AST) structures for all language constructs and implement a basic evaluation engine for simple functional expressions, variable definitions, and lookups within a lexical scope.",
      "details": "AST nodes should cover: literals (numbers, booleans, strings), symbols (variables), function calls `(f arg1 arg2)`, lambda definitions `(lambda (params) body)`, variable definitions `(define x value)`, conditional expressions `(if cond then-expr else-expr)`. Implement an environment model for lexical scoping (e.g., a chain of hash maps). The evaluator will recursively walk the AST. For `(define x value)`, store `value` in the current environment. For symbols, look them up. For `(lambda ...)`, create a closure object capturing the current environment. For function calls, evaluate arguments, extend environment with params, and evaluate body.",
      "testStrategy": "Test evaluation of simple arithmetic and boolean expressions. Test variable definition and lookup in global and local scopes. Test lambda creation and application. Verify correct evaluation of conditional expressions. Test basic error handling for unbound variables.",
      "priority": "medium",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Core AST Node Structures",
          "description": "Implement the data structures for all required Abstract Syntax Tree (AST) nodes: Literals (Number, Boolean, String), Symbol, FunctionCall, Lambda, Define, and If. These structures will represent the parsed code.",
          "dependencies": [],
          "details": "Use classes, structs, or algebraic data types for each AST node. For example: `LiteralNode(value)`, `SymbolNode(name)`, `FunctionCallNode(callee: ASTNode, arguments: List[ASTNode])`, `LambdaNode(params: List[SymbolNode], body: ASTNode)`, `DefineNode(name: SymbolNode, value: ASTNode)`, `IfNode(condition: ASTNode, thenBranch: ASTNode, elseBranch: ASTNode)`. Consider a common base type or interface `ASTNode`.",
          "status": "pending",
          "testStrategy": "Unit test each AST node constructor to ensure correct initialization and storage of its components. Verify that node types can be distinguished."
        },
        {
          "id": 2,
          "title": "Implement Environment Model for Lexical Scoping",
          "description": "Create an `Environment` class or structure to manage variable bindings and lexical scopes. It should support defining variables, looking up variable values, and creating nested scopes.",
          "dependencies": [
            1
          ],
          "details": "Implement the environment as a chain of hash maps (dictionaries). Each `Environment` instance should have a local store for bindings and a reference to an optional parent (outer) environment. Key methods: `define(name: String, value: Any)`, `lookup(name: String) -> Any`, `extend() -> Environment`. `lookup` should search the current environment, then delegate to the parent if not found, throwing an error if the variable is undefined.",
          "status": "pending",
          "testStrategy": "Unit test `Environment` operations: define a variable and look it up in the same scope. Look up a variable defined in an outer scope from an inner scope. Test extending scopes and ensuring inner scope definitions do not affect outer scopes. Test lookup of an undefined variable."
        },
        {
          "id": 3,
          "title": "Implement Basic Evaluator for Literals, Symbols, and `define`",
          "description": "Develop the initial `evaluate` function that takes an AST node and an environment. Implement evaluation logic for Literal nodes, Symbol lookups, and `(define x value)` expressions.",
          "dependencies": [
            1,
            2
          ],
          "details": "The `evaluate(node: ASTNode, env: Environment)` function will use pattern matching or type checking on the `node` to dispatch to specific evaluation logic. \n- Literals: Return their intrinsic value (e.g., number, boolean, string).\n- Symbols: Use `env.lookup(symbolName)` to retrieve the variable's value.\n- `DefineNode`: Evaluate the `value` sub-expression using `evaluate(defineNode.value, env)`. Then, use `env.define(defineNode.name.name, evaluatedValue)`. The `define` expression could return a special confirmation value or the evaluated value.",
          "status": "pending",
          "testStrategy": "Unit test evaluation of: number, string, boolean literals. Test `define`: evaluate `(define x 10)`, then evaluate `x` to check if it returns 10. Test error handling for unbound symbols."
        },
        {
          "id": 4,
          "title": "Implement Evaluator for `lambda` Expressions and Closure Creation",
          "description": "Extend the evaluator to handle `(lambda (params) body)` expressions. This involves creating `Closure` objects that capture the current lexical environment.",
          "dependencies": [
            3
          ],
          "details": "When `evaluate` encounters a `LambdaNode`, it should create a `Closure` object (or an equivalent data structure). This `Closure` must store: \n1. The lambda's parameters (list of symbol names).\n2. The lambda's body (the ASTNode representing the function body).\n3. A reference to the environment active at the point of the lambda's definition (this is crucial for lexical scoping). \nThe `Closure` object itself is the runtime value representing the function.",
          "status": "pending",
          "testStrategy": "Unit test `lambda` evaluation: ensure a `Closure` object is returned. Verify that the closure correctly captures the parameters, body, and the defining environment (e.g., by inspecting the closure or by simple calls in later tests)."
        },
        {
          "id": 5,
          "title": "Implement Evaluator for Function Calls and `if` Expressions",
          "description": "Complete the core evaluation engine by implementing logic for `FunctionCallNode` (evaluating function calls) and `IfNode` (conditional execution).",
          "dependencies": [
            4
          ],
          "details": "For `FunctionCallNode`:\n1. Evaluate the `callee` sub-expression. This should yield a `Closure` (from subtask 4).\n2. Evaluate each `argument` sub-expression in the *current* environment to get argument values.\n3. Create a new `Environment` for the function execution. This new environment's parent should be the `Closure`'s captured environment.\n4. Bind the `Closure`'s parameter names to the evaluated argument values in this new environment.\n5. Evaluate the `Closure`'s body ASTNode within this new, extended environment. The result of this evaluation is the result of the function call.\nFor `IfNode`:\n1. Evaluate the `condition` sub-expression.\n2. If the result is truthy (e.g., not `false`), evaluate and return the result of the `thenBranch`.\n3. Otherwise, evaluate and return the result of the `elseBranch`.",
          "status": "pending",
          "testStrategy": "Unit test `if` expressions: test both true and false conditions. Unit test function calls: \n- Simple call: `((lambda (x) x) 10)` should yield 10.\n- Closure behavior: `(define y 5) (define f (lambda (x) (+ x y))) (f 10)` should yield 15 (assuming `+` is predefined or part of a later task; for now, can test with identity or simple operations).\n- Argument evaluation order (if relevant, though typically left-to-right).\n- Recursive calls if the language structure allows for it (e.g., `(define fac (lambda (n) (if (= n 0) 1 (* n (fac (- n 1)))))) (fac 3)`)."
        }
      ]
    },
    {
      "id": 3,
      "title": "Basic Type System Implementation (Primitive Types & Function Types)",
      "description": "Implement the foundational components of the HoTT-based type system, including representations for basic predefined types (e.g., `Nat`, `Bool`, `String`) and simple function types.",
      "details": "Define data structures to represent types, e.g., `TypeAtom(name)`, `FunctionType(param_types, return_type)`. Predefine instances for `Nat`, `Bool`, `String`. The type system should be able to represent these types and allow for their comparison (equality of types). This does not yet include type checking logic, only the representation.",
      "testStrategy": "Unit tests for creating and representing basic types (`Nat`, `Bool`) and function types (e.g., `(-> Nat Bool)`, `(-> Nat Nat Nat)`). Verify that type representations are distinct and can be compared for equality.",
      "priority": "medium",
      "dependencies": [
        2
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Base 'Type' Interface/Enum",
          "description": "Establish the core data structure or enumeration that will serve as the basis for all type representations in the system. This will define the common contract for different kinds of types.",
          "dependencies": [],
          "details": "Define an abstract base class, interface, or an enum (e.g., using a tagged union or sum type pattern if the language supports it well) named `Type`. This structure should be designed to be extensible for future type kinds (e.g., product types, sum types). For now, it just needs to be a common ancestor or variant type that `TypeAtom` and `FunctionType` can conform to or be variants of.",
          "status": "pending",
          "testStrategy": "Verify that the `Type` definition exists and can be referenced. Static analysis or compilation success is sufficient at this stage."
        },
        {
          "id": 2,
          "title": "Implement 'TypeAtom' Structure for Primitive Types",
          "description": "Create the data structure for representing atomic, predefined types like `Nat`, `Bool`, `String`. This structure will hold the name of the atomic type.",
          "dependencies": [
            1
          ],
          "details": "Define a class or struct `TypeAtom` that inherits from or implements the base `Type` (from subtask 1). It must contain a `name` field (e.g., a string) to store the identifier of the atomic type (e.g., 'Nat', 'Bool'). Ensure it can be instantiated.",
          "status": "pending",
          "testStrategy": "Instantiate `TypeAtom` with different names (e.g., 'Nat', 'TestType'). Verify that the `name` property is correctly stored and accessible. Check that `TypeAtom` instances are considered instances of the base `Type`."
        },
        {
          "id": 3,
          "title": "Implement 'FunctionType' Structure",
          "description": "Create the data structure for representing function types, which consist of a list of parameter types and a single return type.",
          "dependencies": [
            1
          ],
          "details": "Define a class or struct `FunctionType` that inherits from or implements the base `Type` (from subtask 1). It should contain: \n1. `parameter_types`: A list or array of `Type` instances representing the types of the function's parameters. \n2. `return_type`: A `Type` instance representing the function's return type. \nEnsure it can be instantiated with appropriate `Type` instances (which could be `TypeAtom` or other `FunctionType` instances) for its fields.",
          "status": "pending",
          "testStrategy": "Instantiate `FunctionType` using mock `Type` instances (or `TypeAtom` instances if subtask 2 is testable in isolation first) for parameter and return types. For example, `FunctionType([mockType1, mockType2], mockReturnType)`. Verify that `parameter_types` and `return_type` are correctly stored and accessible. Check that `FunctionType` instances are considered instances of the base `Type`."
        },
        {
          "id": 4,
          "title": "Predefine Constant Instances for Nat, Bool, String",
          "description": "Create and store globally accessible constant instances for the primitive types `Nat`, `Bool`, and `String` using the `TypeAtom` structure.",
          "dependencies": [
            2
          ],
          "details": "In a suitable module or globally accessible scope, define constant instances of `TypeAtom` for 'Nat', 'Bool', and 'String'. For example: `const TYPE_NAT = new TypeAtom('Nat');`, `const TYPE_BOOL = new TypeAtom('Bool');`, `const TYPE_STRING = new TypeAtom('String');`. These should be readily available for use throughout the type system.",
          "status": "pending",
          "testStrategy": "Verify that the predefined instances `TYPE_NAT`, `TYPE_BOOL`, `TYPE_STRING` exist, are of type `TypeAtom` (and thus `Type`), and have the correct names ('Nat', 'Bool', 'String' respectively)."
        },
        {
          "id": 5,
          "title": "Implement Type Equality Comparison Logic",
          "description": "Implement a mechanism to compare two type instances for structural equality. This must handle `TypeAtom` and `FunctionType` correctly.",
          "dependencies": [
            2,
            3
          ],
          "details": "Implement an equality comparison method or function that can take two `Type` instances. \n- For `TypeAtom`: Two `TypeAtom` instances are equal if their `name` fields are identical (case-sensitive). \n- For `FunctionType`: Two `FunctionType` instances are equal if: \n    1. They have the same number of parameter types. \n    2. Each corresponding parameter type is equal (recursively using this equality logic). \n    3. Their return types are equal (recursively). \n- If comparing types of different kinds (e.g., `TypeAtom` vs `FunctionType`), they are not equal. \nThis comparison should be part of the `Type` interface/base class or a dispatching utility function.",
          "status": "pending",
          "testStrategy": "Test cases using predefined types (from subtask 4) and custom ones: \n1. `TYPE_NAT` == `TYPE_NAT`. \n2. `TYPE_NAT` != `TYPE_BOOL`. \n3. `new TypeAtom('Nat')` == `TYPE_NAT`. \n4. `FunctionType([TYPE_NAT], TYPE_BOOL)` == `FunctionType([TYPE_NAT], TYPE_BOOL)`. \n5. `FunctionType([TYPE_NAT], TYPE_BOOL)` != `FunctionType([TYPE_NAT], TYPE_STRING)`. \n6. `FunctionType([TYPE_NAT], TYPE_BOOL)` != `FunctionType([TYPE_STRING], TYPE_BOOL)`. \n7. `FunctionType([TYPE_NAT, TYPE_BOOL], TYPE_STRING)` != `FunctionType([TYPE_NAT], TYPE_STRING)`. \n8. `TYPE_NAT` != `FunctionType([], TYPE_NAT)`."
        }
      ]
    },
    {
      "id": 4,
      "title": "Type Checker for Simple Types and Functions",
      "description": "Develop a type checker that can validate expressions involving basic types and simple (non-dependent) function definitions and applications. It should infer types where possible and report type errors.",
      "details": "Implement a type checking algorithm, potentially bidirectional (checking and inference). For `(define x value)`, infer type of `value` and assign to `x`. For `(lambda (p1 p2) body)`, if param types are annotated, use them; otherwise, attempt inference. Type check `body` against expected return type. For `(f arg1 arg2)`, check that `f` is a function, and `arg` types match `f`'s param types. The type checker will operate on the AST and use the type representations from Task 3. Error messages should be clear about type mismatches.",
      "testStrategy": "Test with well-typed programs (e.g., `(define (add (x Nat) (y Nat)) (+ x y))`, `(add 1 2)`). Test with ill-typed programs (e.g., applying a number as a function, wrong argument types, incorrect return type in function body). Verify correct type inference for simple cases and accurate error reporting for mismatches.",
      "priority": "medium",
      "dependencies": [
        3
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Type Checking for Literals and Variable References",
          "description": "Develop the core logic to determine the types of basic literals (e.g., integers, booleans) and to look up types of variables from a type environment. This forms the foundation for checking more complex expressions.",
          "dependencies": [],
          "details": "Create a `TypeEnvironment` class/structure to store variable-to-type mappings (scoped, if necessary, for later features like let-bindings or nested functions). Implement a `check(ast_node, environment)` or `infer(ast_node, environment)` method. For literal AST nodes (e.g., `NumberNode`, `BooleanNode`), this method should return their corresponding `Type` objects (e.g., `NumberType`, `BooleanType` from Task 3). For `VariableNode`, it should look up the variable's name in the `TypeEnvironment`. If not found, this is an unbound variable situation (actual error reporting will be refined in a later subtask, but the condition should be detectable).",
          "status": "pending",
          "testStrategy": "Test with AST nodes for various literals (e.g., `1`, `true`). Test variable lookups in environments with and without the variable defined. Ensure correct `Type` objects are returned."
        },
        {
          "id": 2,
          "title": "Implement Type Checking for `define` Expressions",
          "description": "Extend the type checker to handle `(define x value)` expressions. This involves inferring the type of the `value` expression and updating the type environment with this new binding for `x`.",
          "dependencies": [
            1
          ],
          "details": "For a `DefineNode(name, value_expr)`: 1. Recursively call the type checker (e.g., `infer(value_expr, environment)`) on the `value_expr` to determine its type. 2. If `value_expr` type checking fails, propagate the error. 3. Otherwise, update the current `TypeEnvironment` by adding a mapping from `name` (a string) to the inferred type. The `define` expression itself might be considered to have a special `Void` type or the type of the variable being defined; clarify this or focus on the environment side-effect. Ensure the updated environment is used for subsequent expressions at the same scope.",
          "status": "pending",
          "testStrategy": "Test `(define x 10)` and verify that `x` is subsequently typed as `NumberType`. Test `(define y true)` for `BooleanType`. Test `(define z x)` after `x` is defined, ensuring `z` gets `x`'s type. Test cases where `value` expression has a type error."
        },
        {
          "id": 3,
          "title": "Implement Type Checking for `lambda` Expressions (Function Definitions)",
          "description": "Implement type checking for function definitions `(lambda (param1 param2 ...) body)`. This includes processing parameter type annotations, creating a new scope for the function body, and inferring the function's return type by checking the body. The result is a function type.",
          "dependencies": [
            1
          ],
          "details": "For a `LambdaNode(params, body_expr)`: 1. Create a new, extended `TypeEnvironment` that inherits from the current environment. 2. For each parameter in `params`: If type annotations are present (e.g., `(x : Int)`), add the parameter name and its annotated type to this new environment. If annotations are absent, for this initial implementation, either require annotations or assume a default 'Unknown/Any' type, or prepare for more complex inference if specified. 3. Recursively call the type checker (e.g., `infer(body_expr, new_environment)`) on `body_expr` to determine its type. This is the function's return type. 4. The type of the `lambda` expression itself is a `FunctionType` (from Task 3) composed of the list of parameter types and the inferred return type.",
          "status": "pending",
          "testStrategy": "Test `(lambda ((x : Int)) x)` should result in a `FunctionType(param_types=[IntType], return_type=IntType)`. Test `(lambda ((a : Bool) (b : Int)) b)` resulting in `FunctionType(param_types=[BoolType, IntType], return_type=IntType)`. Test cases where the body has a type error based on its own structure or use of parameters."
        },
        {
          "id": 4,
          "title": "Implement Type Checking for Function Applications",
          "description": "Implement type checking for function application expressions `(f arg1 arg2 ...)`. This involves ensuring `f` evaluates to a function type, the number of arguments matches the function's arity, and each argument's type is compatible with the corresponding parameter type. The type of the application is the function's return type.",
          "dependencies": [
            1,
            3
          ],
          "details": "For an `ApplicationNode(function_expr, arg_exprs)`: 1. Recursively call the type checker on `function_expr` to get its type. Verify this is a `FunctionType`. If not, it's a type error ('not a function'). 2. Compare the number of `arg_exprs` with the arity (number of parameters) of the `FunctionType`. If they don't match, it's an arity mismatch error. 3. For each `arg_expr` and its corresponding parameter type from the `FunctionType`: a. Recursively call the type checker on `arg_expr` to get its type. b. Check if the argument's actual type is assignable to/compatible with the expected parameter type. If not, it's a type mismatch error. 4. If all checks pass, the type of the `ApplicationNode` expression is the return type specified in the `FunctionType` of `function_expr`.",
          "status": "pending",
          "testStrategy": "Given `(define myfun (lambda ((a : Int) (b : Bool)) a))`: Test `(myfun 1 true)` should type check to `IntType`. Test `(myfun 1 2)` (argument type mismatch). Test `(myfun 1)` (arity mismatch). Test `(1 2 3)` ('not a function' error for `1`)."
        },
        {
          "id": 5,
          "title": "Implement Clear Type Error Reporting and System Integration",
          "description": "Develop a robust error reporting mechanism that generates clear, user-friendly messages for all detected type errors (e.g., mismatches, unbound variables, arity errors). Integrate the type checker to operate on a full AST and collect all errors.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "1. Define a `TypeError` class or structure to encapsulate error details: message, AST node location (for line/column numbers if available), expected type, actual type, etc. 2. Modify all type checking functions (from subtasks 1-4) to generate and return/collect these `TypeError` objects when an issue is detected, instead of throwing exceptions or immediately halting (to allow for collecting multiple errors). 3. Create a main driver function for the type checker that traverses the entire program AST. This driver should invoke the appropriate checking logic for each node type and accumulate any `TypeError` objects. 4. Format error messages clearly, e.g., \"Type Error: Expected 'Int' but found 'Bool' for parameter 'y' in function call 'f'. (line X, col Y)\". Ensure messages distinguish between different kinds of errors.",
          "status": "pending",
          "testStrategy": "Create a suite of small programs, each designed to trigger specific type errors: unbound variable, type mismatch in definition, type mismatch in function argument, calling a non-function, incorrect arity in function call. Verify that the type checker identifies all errors and that the reported messages are accurate, informative, and (if possible) point to the source location."
        }
      ]
    },
    {
      "id": 5,
      "title": "Canonical Instantiation Functions (CIFs) - Pure Product and Sum Types",
      "description": "Implement the `deftype` syntax for defining simple product types (struct-like) and sum types (enum-like). Generate and evaluate pure Canonical Instantiation Functions (CIFs) for these types. No effects or complex constraints at this stage.",
      "details": "Extend parser for `(deftype Name (fields...) ...)` for product types (e.g., `(deftype Point ((x Nat) (y Nat)))`) and `(deftype Name (Variant1 type1) (Variant2 type2) ...)` for sum types (e.g., `(deftype Shape (Circle Nat) (Rectangle Nat Nat)))`). The type checker must register these new types. CIFs are implicitly defined by the type name (e.g., `(Point 1 2)`, `(Circle 5)`). The evaluator needs to handle instantiation of these custom types, creating record-like or tagged-union-like data structures. CIFs are pure at this stage.",
      "testStrategy": "Define several product and sum types. Instantiate them using their CIFs. Verify that the type checker correctly types these instantiations. Access fields of product types and pattern match (rudimentary, if available, or inspect structure) on sum types. Ensure instantiations are evaluated to correct runtime values.",
      "priority": "medium",
      "dependencies": [
        4
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend Parser for `deftype` Syntax (Product and Sum Types)",
          "description": "Modify the parser to recognize and parse the `(deftype Name ...)` syntax for both product types `(deftype Name ((field1 type1) (field2 type2) ...))` and sum types `(deftype Name (Variant1 typeA ...) (Variant2 typeB typeC ...) ...)`. The parser should generate distinct Abstract Syntax Tree (AST) nodes for these definitions.",
          "dependencies": [],
          "details": "Implement parsing rules for:\n1. Product types: `(deftype <TypeName> (<fieldName> <fieldType>)+)`. Example: `(deftype Point ((x Nat) (y Nat)))`. The AST node should capture the type name, and an ordered list of (field name, field type name) pairs.\n2. Sum types: `(deftype <TypeName> (<VariantName> <fieldType>*)*)`. Example: `(deftype Shape (Circle Nat) (Rectangle Nat Nat))`. The AST node should capture the type name, and a list of (variant name, list of argument type names) pairs.\nEnsure robust error handling for malformed definitions.",
          "status": "pending",
          "testStrategy": "Unit test the parser with various valid `deftype` expressions for both product and sum types. Test edge cases like empty field lists (if disallowed), empty variant lists, types with no arguments for variants. Verify correct AST node structure and content. Test with invalid syntax to ensure proper error reporting."
        },
        {
          "id": 2,
          "title": "Type Checker: Register User-Defined Product and Sum Types",
          "description": "Enhance the type checker to process `deftype` AST nodes from the parser. It must register these new product and sum types in the type environment, storing their structure (field names and resolved types for products; variant names and their resolved argument types for sums).",
          "dependencies": [
            1
          ],
          "details": "When a `deftype` AST node is encountered during type checking:\n1. For product types: Resolve all field types against the current type environment. Store the new type name along with its definition (e.g., an ordered list of (field name, resolved field type)).\n2. For sum types: Resolve all argument types for each variant against the current type environment. Store the new type name along with its definition (e.g., a map of variant names to a list of resolved argument types).\n3. Check for type name collisions within the current scope. Disallow redefinition of existing types or primitive types.\n4. The registered types should be available for subsequent type checking.",
          "status": "pending",
          "testStrategy": "Unit test the type checker by processing various `deftype` AST nodes. Verify that types are correctly registered in the type environment with their accurate structure. Test scenarios with valid type references, undefined type references in field/variant definitions, and type name conflicts."
        },
        {
          "id": 3,
          "title": "Type Checker: Recognize and Type Check Canonical Instantiation Function (CIF) Calls",
          "description": "Update the type checker to recognize expressions like `(TypeName arg1 ...)` or `(VariantName arg1 ...)` as Canonical Instantiation Function (CIF) calls. It should type-check these calls against the registered product or sum type definitions, determining the type of the resulting expression.",
          "dependencies": [
            2
          ],
          "details": "1. For product types: If an application form `(F arg1 ... argN)` is encountered and `F` matches a registered product type name (e.g., `Point`), treat this as a CIF call.\n   - Verify the number of arguments matches the number of fields in the `Point` type definition.\n   - Type-check each argument `argI` against the corresponding field type `fieldTypeI`.\n   - The type of the entire expression `(Point arg1 ... argN)` is `Point`.\n2. For sum types: If an application form `(V arg1 ... argM)` is encountered and `V` matches a registered variant name (e.g., `Circle` of type `Shape`), treat this as a CIF call.\n   - Verify `V` is a known variant of some sum type `S`.\n   - Verify the number of arguments matches the definition of variant `V`.\n   - Type-check each argument `argI` against the corresponding argument type for variant `V`.\n   - The type of the entire expression `(Circle arg1 ... argM)` is the parent sum type `S` (e.g., `Shape`).\nCIFs are implicit; no explicit function definition is parsed for them.",
          "status": "pending",
          "testStrategy": "Unit test the type checker with various CIF calls. Include tests for: \n- Correct product type instantiations (e.g., `(Point 1 2)`).\n- Correct sum type variant instantiations (e.g., `(Circle 5)`).\n- Incorrect calls: wrong number of arguments, arguments of incorrect types for both product and sum CIFs.\n- Calls to undefined type names or variant names."
        },
        {
          "id": 4,
          "title": "Evaluator: Implement Product Type Instantiation",
          "description": "Modify the evaluator to handle CIF calls for product types. When a type-checked expression like `(Point 1 2)` is evaluated, it should create and return an internal runtime representation of a product type instance (e.g., a record or struct-like object).",
          "dependencies": [
            3
          ],
          "details": "1. When the evaluator encounters a CIF call for a product type (identified by the type checker, e.g., an AST node tagged as 'ProductCIFCall' with type name and arguments):\n   - Evaluate each argument expression to get its runtime value.\n   - Construct an internal data structure for the product instance. This structure should store the evaluated argument values, associated with their field names (e.g., a map like `{\"x\": <value_of_arg1>, \"y\": <value_of_arg2>}`) or in a fixed order if field names are not stored at runtime for pure products.\n   - The runtime instance should also be tagged with its type name (e.g., `Point`).\n   - Example: `(Point 1 2)` evaluates to a runtime value representing `{ type: 'Point', fields: { x: 1, y: 2 } }` or similar.",
          "status": "pending",
          "testStrategy": "Unit test the evaluator for product type instantiation. \n- Evaluate simple product CIF calls like `(Point 1 2)` and verify the structure and content of the resulting runtime object.\n- Test with different data types for fields (e.g., numbers, booleans, other custom types if supported).\n- Ensure the runtime object is correctly tagged with its type."
        },
        {
          "id": 5,
          "title": "Evaluator: Implement Sum Type Instantiation",
          "description": "Modify the evaluator to handle CIF calls for sum type variants. When a type-checked expression like `(Circle 5)` (for `deftype Shape (Circle Nat) ...`) is evaluated, it should create and return an internal runtime representation of a sum type instance (e.g., a tagged union).",
          "dependencies": [
            3
          ],
          "details": "1. When the evaluator encounters a CIF call for a sum type variant (identified by the type checker, e.g., an AST node tagged as 'SumCIFCall' with sum type name, variant name, and arguments):\n   - Evaluate each argument expression to get its runtime value.\n   - Construct an internal data structure for the sum type instance. This structure must include:\n     a. The specific variant tag/name (e.g., `\"Circle\"`).\n     b. The evaluated argument values for that variant (e.g., `[<value_of_arg1>]`).\n     c. The overall sum type name (e.g., `\"Shape\"`).\n   - Example: `(Circle 5)` for `(deftype Shape (Circle Nat) ...)` evaluates to a runtime value like `{ type: 'Shape', variant: 'Circle', values: [5] }`.\n   - Example: `(Rectangle 10 20)` for `(deftype Shape ... (Rectangle Nat Nat))` evaluates to `{ type: 'Shape', variant: 'Rectangle', values: [10, 20] }`.",
          "status": "pending",
          "testStrategy": "Unit test the evaluator for sum type instantiation.\n- Evaluate various sum type variant CIF calls (e.g., `(Circle 5)`, `(Rectangle 10 20)`).\n- Verify the structure and content of the resulting tagged runtime object (correct sum type, variant tag, and values).\n- Test variants with no arguments and multiple arguments.\n- Ensure the runtime object is correctly tagged."
        }
      ]
    },
    {
      "id": 6,
      "title": "Basic REPL Implementation",
      "description": "Create a basic Read-Eval-Print Loop (REPL) for interactive development, integrating the parser, type checker, and evaluator. This provides an initial user interface for the language.",
      "details": "The REPL should: Read a line of user input. Parse the input S-expression. Type check the parsed AST. If type checking succeeds, evaluate the AST. Print the result of evaluation or any error messages (parse error, type error, runtime error). Maintain a persistent environment across REPL inputs for definitions.",
      "testStrategy": "Interactive testing: Define types using `deftype`. Define functions. Instantiate types. Evaluate expressions involving these definitions. Check error handling and reporting within the REPL for all stages (read, type check, eval). Verify environment persistence.",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement REPL Core Loop and User Input Reading",
          "description": "Set up the fundamental REPL structure that continuously prompts the user for input, reads a line of text, and handles basic exit commands (e.g., 'exit', 'quit'). This forms the basic interaction shell.",
          "dependencies": [],
          "details": "Implement an infinite loop that: 1. Prints a prompt (e.g., '> '). 2. Reads a line of text from standard input. 3. Checks if the input is an exit command (e.g., 'exit', ':q'). If so, terminate the REPL. Otherwise, proceed to the next step (parsing, to be added in a subsequent subtask). Use appropriate I/O functions for the target language/environment.",
          "status": "pending",
          "testStrategy": "Manually run the REPL. Verify that it displays a prompt. Enter text and see it acknowledged (even if not processed yet). Enter exit commands and verify the REPL terminates."
        },
        {
          "id": 2,
          "title": "Integrate S-expression Parser and Handle Parse Errors",
          "description": "Connect the existing S-expression parser to the REPL. For each user input (that is not an exit command), attempt to parse it. If parsing fails, display a clear, user-friendly parse error message. If successful, the AST should be available for the next stage.",
          "dependencies": [
            1
          ],
          "details": "After reading input in the REPL loop (from subtask 1), pass the input string to the `parse()` function from the parser module. Implement try-catch or error-checking around the parse call. If a parse error occurs, print a message like 'Parse Error: [error details]' to standard error and loop back for new input. If successful, store the generated AST.",
          "status": "pending",
          "testStrategy": "Enter valid S-expressions; verify no error is printed and the REPL awaits next input (or moves to a placeholder for next stage). Enter malformed S-expressions (e.g., `(foo`, `(bar . )`, `())`) and verify specific parse error messages are displayed."
        },
        {
          "id": 3,
          "title": "Integrate Type Checker and Handle Type Errors",
          "description": "Integrate the type checker module into the REPL. After successful parsing, pass the AST to the type checker. If type checking fails, display a clear type error message. If successful, the (potentially annotated) AST proceeds to evaluation. For this subtask, the type checker will operate with a fresh/empty environment for each input.",
          "dependencies": [
            2
          ],
          "details": "After a successful parse (from subtask 2), call the `typeCheck(ast, initialEnvironment)` function from the type checker module. `initialEnvironment` will be a new, empty environment for each call in this subtask. Implement try-catch or error-checking for type errors. If a type error occurs, print 'Type Error: [error details]' and loop back. If successful, store the (typed) AST.",
          "status": "pending",
          "testStrategy": "Enter syntactically correct but type-incorrect expressions (e.g., `(+ 1 #t)` if booleans aren't summable with numbers). Verify specific type error messages. Enter type-correct expressions and verify no error is printed."
        },
        {
          "id": 4,
          "title": "Integrate Evaluator, Handle Runtime Errors, and Print Results",
          "description": "Integrate the evaluator module. After successful type checking, pass the AST to the evaluator. Print the result of the evaluation. If a runtime error occurs, display a clear runtime error message. For this subtask, the evaluator will operate with a fresh/empty environment for each input (matching the type checker).",
          "dependencies": [
            3
          ],
          "details": "After successful type checking (from subtask 3), call the `evaluate(ast, initialEnvironment)` function from the evaluator module. `initialEnvironment` will be a new, empty environment. Implement try-catch or error-checking for runtime errors. If evaluation is successful, print the resulting value to standard output. If a runtime error occurs (e.g., division by zero, unbound variable before persistence), print 'Runtime Error: [error details]'. Always loop back for new input.",
          "status": "pending",
          "testStrategy": "Enter expressions that evaluate successfully (e.g., `(+ 1 2)`) and verify the correct result is printed. Enter expressions causing runtime errors (e.g., `(/ 1 0)`) and verify specific runtime error messages."
        },
        {
          "id": 5,
          "title": "Implement Persistent Environment Across REPL Inputs",
          "description": "Modify the REPL to initialize and maintain a single, persistent environment that is used across all user inputs. This allows definitions (e.g., variables, functions) made in one REPL interaction to be available in subsequent interactions.",
          "dependencies": [
            4
          ],
          "details": "1. Initialize a global/shared environment object once when the REPL starts. 2. Modify the calls to `typeCheck` (subtask 3) and `evaluate` (subtask 4) to pass this persistent environment instance instead of a fresh one. 3. Ensure that language constructs that create definitions (e.g., `define`, `let` if it modifies the top-level) update this shared environment. The type checker and evaluator might need internal adjustments to correctly handle and update the passed-in environment.",
          "status": "pending",
          "testStrategy": "1. Define a variable: `(define x 10)`. Verify no error and prompt returns. 2. Use the variable: `(+ x 5)`. Verify result is `15`. 3. Define a function: `(define (add_one y) (+ y 1))`. 4. Call the function: `(add_one 20)`. Verify result is `21`. Test redefinition if supported."
        }
      ]
    },
    {
      "id": 7,
      "title": "Algebraic Effect System Core: Declaration and Basic Handling",
      "description": "Implement the core infrastructure for algebraic effects, including effect declaration (`defeffect`) and a basic mechanism for performing effects and handling them with `try/handle` (or `try/catch` for specific effects).",
      "details": "Parse `(defeffect EffectName (operation (params) : ReturnType) ...)` to define effects and their operations. Implement a `perform (EffectName.operation args)` primitive that suspends computation. Implement `(try body-expr (:handle (EffectName op) (k arg) handler-body) ...)` where `k` is the continuation. The handler body can resume the computation using `(resume k value)`. This requires a continuation-passing style transformation or similar mechanism in the evaluator.",
      "testStrategy": "Define a simple custom effect (e.g., `State` with `get`/`put` operations). Write programs that perform these effects. Implement handlers that manage the effect (e.g., provide state). Verify that effects suspend computation and handlers can resume them, demonstrating non-local control flow.",
      "priority": "medium",
      "dependencies": [
        2
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement `defeffect` Parser and Effect Registry",
          "description": "Parse `(defeffect EffectName (operation (params) : ReturnType) ...)` syntax. Store effect definitions, including their operations, parameters, and return types, in a central registry accessible by the runtime.",
          "dependencies": [],
          "details": "Modify the language parser to recognize the `defeffect` keyword and its structure (EffectName, list of operations, each with parameters and an optional ReturnType). Create data structures to represent `EffectDefinition` and `OperationSignature`. Implement a global or module-scoped registry (e.g., a hash map) to store these definitions, keyed by `EffectName`. Each `EffectName` would map to an object/struct containing its defined operations and their signatures. Ensure validation of effect and operation names (e.g., uniqueness, valid identifiers).",
          "status": "pending",
          "testStrategy": "Define several effects with varying numbers of operations, different parameter lists, and with/without return types. Verify that they are parsed correctly and stored in the registry with the expected structure. Test error handling for malformed `defeffect` expressions (e.g., duplicate effect names, invalid operation syntax)."
        },
        {
          "id": 2,
          "title": "Implement `perform` Primitive and Initial Suspension Signal",
          "description": "Implement the `(perform EffectName.operation args)` primitive. When `perform` is called, it should identify the effect and operation, package the arguments, and signal a suspension of the current computation by propagating an effect instance.",
          "dependencies": [
            1
          ],
          "details": "Add `perform` as a built-in function or special form. It should look up `EffectName.operation` in the registry (from subtask 1) to validate the operation and arity of `args`. Create a unique, identifiable object (e.g., an \"EffectInstance\" or a special throwable object) that encapsulates the specific effect operation being performed and its arguments. The evaluator must be modified to recognize this signal and halt normal execution flow, propagating this signal upwards. This signal will later be caught by a `try/handle` block.",
          "status": "pending",
          "testStrategy": "Test `perform` with valid effect/operation names and correct arguments. Verify that performing an effect interrupts the normal flow and propagates an EffectInstance object. Test error conditions: unknown effect/operation, incorrect number of arguments. If no handler is present, this might result in an unhandled effect error."
        },
        {
          "id": 3,
          "title": "Implement `try/handle` Syntax Parsing and Handler Scope Setup",
          "description": "Parse the `(try body-expr (:handle (EffectName op) (k arg) handler-body) ...)` syntax. During evaluation of a `try` expression, establish a dynamic scope that registers the provided handlers for specific effects and operations.",
          "dependencies": [
            1
          ],
          "details": "Extend the parser to support the `try` special form and its `:handle` clauses. Each `:handle` clause specifies an `EffectName`, an `operation` name (`op`), a continuation parameter (`k`), an argument parameter (`arg`), and a `handler-body`. When a `try` expression is evaluated, push a new frame onto a runtime \"handler stack\". This frame should store a mapping from `(EffectName, operationName)` pairs to their corresponding handler functions (closures created from `handler-body` capturing `k` and `arg` parameters and the lexical environment). The `body-expr` is then evaluated within this scope. Upon normal completion or an effect being handled and resumed out of the `try` block, this handler frame should be popped.",
          "status": "pending",
          "testStrategy": "Parse various `try/handle` expressions with single and multiple `:handle` clauses. Verify that the handler scope is correctly established on the handler stack during `body-expr` evaluation and torn down afterwards. Test evaluation of `body-expr` within a `try` block that does not perform any effects, ensuring it executes normally."
        },
        {
          "id": 4,
          "title": "Implement Effect Handling Logic and Continuation Capturing",
          "description": "Integrate `perform` with `try/handle`. When an effect is signaled (from subtask 2) within a `try` block, search the handler stack for a matching handler. If found, capture the current continuation and invoke the handler with the continuation and effect arguments.",
          "dependencies": [
            2,
            3
          ],
          "details": "Modify the evaluator's response to the \"EffectInstance\" signal. When an effect is signaled, the evaluator should search the current handler stack (from subtask 3) for a handler matching the `EffectName` and `operationName` from the EffectInstance. If a handler is found: 1. Capture the current state of computation as a \"continuation\" object/function. This continuation, when invoked later with a value, should resume execution as if the `perform` call returned that value. 2. Invoke the matched handler function, passing it the captured continuation (`k`) and the arguments from the `perform` call (bound to `arg`). If no handler is found, the effect signal should continue propagating upwards.",
          "status": "pending",
          "testStrategy": "Create test cases where `perform` is called inside a `try/handle` block with a matching handler. Verify that the correct handler is invoked. Verify that the continuation object and effect arguments are correctly passed to the handler. Initially, the handler might just print arguments or return a fixed value without using `resume`. Test unhandled effects propagating out of a `try` block."
        },
        {
          "id": 5,
          "title": "Implement `resume` Primitive for Continuation Invocation",
          "description": "Implement the `(resume k value)` primitive. This primitive takes a continuation (captured in subtask 4) and a value, and resumes the suspended computation, making the original `perform` call appear to return `value`.",
          "dependencies": [
            4
          ],
          "details": "Add `resume` as a built-in function or primitive. It expects a continuation object (as created in subtask 4) and a result value. Invoking `(resume k value)` should: 1. Restore the evaluation context (e.g., stack, relevant parts of the environment) associated with the continuation. 2. Make the original `perform` call (that was suspended) evaluate to `value`. 3. Ensure that the handler stack is correctly managed; the handler that called `resume` is typically no longer active for the resumed computation unless it re-establishes handlers. The control flow returns to the point after the `perform` call.",
          "status": "pending",
          "testStrategy": "Test handlers that use `(resume k some-value)`. Verify that the computation resumes correctly from the point of `perform` and that the `perform` expression effectively returns `some-value`. Test scenarios with multiple `perform` calls for the same effect handled by the same handler instance (if the handler design allows re-entry or multiple resumptions). Test resuming with different types of values. Ensure that after resumption, the execution context is correct."
        }
      ]
    },
    {
      "id": 8,
      "title": "`ConstructionFailure` Effect and Basic CIF Runtime Constraints",
      "description": "Introduce the predefined `ConstructionFailure` effect. Modify CIFs for types defined with `(:where ...)` clauses to perform this effect if runtime constraint checks fail.",
      "details": "Predefine the `ConstructionFailure` effect (e.g., `(defeffect ConstructionFailure (fail (message String)) : Nothing)`). Extend `deftype` to parse simple `(:where predicate-expr)` clauses. When a CIF for such a type is called, evaluate the `predicate-expr` after constructing the underlying value. If the predicate is false, `perform (ConstructionFailure.fail \"Constraint violated\")`. Users should be able to catch this with `(try (MyType ...) (:catch (e : ConstructionFailure) fallback))`. The `:catch` syntax is a specialized handler for exceptions.",
      "testStrategy": "Define a type with a `(:where ...)` constraint (e.g., `(deftype PositiveNat Nat (:where (> self 0)))`). Test instantiation with valid values (should succeed) and invalid values (should perform `ConstructionFailure`). Verify that `try/catch` can intercept `ConstructionFailure` and execute fallback logic.",
      "priority": "medium",
      "dependencies": [
        5,
        7
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Predefined `ConstructionFailure` Effect",
          "description": "Introduce the `ConstructionFailure` effect into the system's core library or prelude. This effect signals failures during type construction when runtime constraints are violated.",
          "dependencies": [],
          "details": "Implement `(defeffect ConstructionFailure (fail (message String)) : Nothing)` as specified. Ensure it's available globally. The `fail` operation should accept a `String` message. The effect type should indicate no return value (e.g., `: Nothing` or equivalent mechanism if the language uses a different syntax for effect signatures).",
          "status": "pending",
          "testStrategy": "Verify `ConstructionFailure` is a recognized effect type. Test that `(perform (ConstructionFailure.fail \"test message\"))` can be invoked (it might initially be unhandled). Check that the `fail` operation correctly packages the message as its payload."
        },
        {
          "id": 2,
          "title": "Extend `deftype` to Parse and Store `:where` Clauses",
          "description": "Modify the `deftype` macro or special form to recognize, parse, and store `(:where predicate-expr)` clauses found within type definitions. The predicate expression will be used for runtime validation.",
          "dependencies": [
            1
          ],
          "details": "Update the `deftype` parser. When a `(:where predicate-expr)` clause is encountered: 1. Parse `predicate-expr`. 2. Store this predicate (e.g., as a lambda, an abstract syntax tree, or a compiled function) in the type's metadata. 3. Define and document a convention for how the value being constructed will be referenced within `predicate-expr` (e.g., a special variable like `%`, `this`, or as the first argument to a lambda).",
          "status": "pending",
          "testStrategy": "Define various types using `deftype` with and without `:where` clauses. Inspect the internal representation (e.g., type metadata object) to confirm correct parsing and storage of the predicate. Test with syntactically valid and invalid predicates to ensure robust parsing."
        },
        {
          "id": 3,
          "title": "Adapt CIF Generation to Incorporate Constraint Predicates",
          "description": "Update the Constructor Invocation Function (CIF) generation logic. For types defined with a `:where` clause, the generated CIF must be structured to call the stored predicate expression after the base value is constructed.",
          "dependencies": [
            2
          ],
          "details": "Modify the code generation part of `deftype` or the CIF creation mechanism. When a CIF is generated for a type with a stored predicate: 1. The CIF should first construct the underlying value as usual. 2. After construction, the generated CIF code should include a call to the stored predicate, passing the newly constructed value to it according to the convention established in Subtask 2. The actual performing of the `ConstructionFailure` effect based on the predicate's result is handled in the next subtask.",
          "status": "pending",
          "testStrategy": "For types with `:where` clauses, inspect the generated CIF code (if feasible) or use a debugger to verify that the structure for predicate evaluation (i.e., the call to the predicate function) is correctly inserted into the CIF. This subtask focuses on setting up the call, not the full execution and effect performance yet."
        },
        {
          "id": 4,
          "title": "Implement Runtime Predicate Check and `ConstructionFailure` Invocation in CIFs",
          "description": "Enhance CIFs for types with `:where` clauses to execute the stored predicate at runtime. If the predicate evaluates to false, the CIF must perform the `ConstructionFailure.fail` effect.",
          "dependencies": [
            1,
            3
          ],
          "details": "In the CIF execution flow for types with constraints: 1. After the base value is constructed and the predicate is called (as set up in Subtask 3), check its return value. 2. If the predicate returns `false` (or any falsey value according to the language's semantics), perform `(ConstructionFailure.fail \"Constraint violated\")`. The message can be made more specific, e.g., by including the type name or a user-provided message from the `:where` clause if supported.",
          "status": "pending",
          "testStrategy": "Instantiate types with `:where` clauses: 1. For predicates that should pass, verify successful construction and correct value. 2. For predicates that should fail, verify that `(ConstructionFailure.fail ...)` is performed. This might initially result in an unhandled effect error if Subtask 5 is not yet complete, which is acceptable at this stage. Confirm the message in the performed effect."
        },
        {
          "id": 5,
          "title": "Implement `try/:catch` Syntax for `ConstructionFailure` Effect",
          "description": "Implement the `(try expression (:catch (e : ConstructionFailure) handler-expression))` syntax. This allows user code to catch the `ConstructionFailure` effect and execute custom handling logic.",
          "dependencies": [
            4
          ],
          "details": "1. Extend the language parser to recognize the `try ... (:catch (variable : EffectType) handler-expression)` syntax, specifically for `EffectType` being `ConstructionFailure`. 2. Implement the runtime semantics: When `expression` is evaluated, if it (or any function it calls) performs an effect of type `ConstructionFailure`, the evaluation of `expression` is aborted. 3. The `handler-expression` is then evaluated. The `variable` (e.g., `e`) should be bound to the payload of the `ConstructionFailure` effect (i.e., the instance returned by `(ConstructionFailure.fail message)`, which should contain the message). 4. The result of the `try` expression in case of a caught effect is the result of the `handler-expression`. If no matching effect is performed, the result is that of `expression`. Ensure this handler is specific to `ConstructionFailure` as per the specialized nature of `:catch` for exceptions.",
          "status": "pending",
          "testStrategy": "1. Test `(try (MyTypeWithFailingConstraint ...) (:catch (e : ConstructionFailure) \"fallback\"))` returns \"fallback\". 2. Verify `e` in the catch block is correctly bound to the effect payload (e.g., an object/struct containing the message string). 3. Test that `(try (MyTypeWithPassingConstraint ...) (:catch (e : ConstructionFailure) \"fallback\"))` returns the successfully constructed type instance and does not execute the handler. 4. Test that other unhandled effects (if any exist in the language) within `try` still propagate if they are not `ConstructionFailure`. 5. Test nested `try/catch` blocks."
        }
      ]
    },
    {
      "id": 9,
      "title": "CIF Purity Analysis and Effect Inference (Initial)",
      "description": "Implement initial logic for CIF purity analysis. The compiler/type checker should infer if a CIF is pure or potentially effectful (e.g., due to `ConstructionFailure` from a runtime constraint).",
      "details": "During `deftype` processing, analyze its definition. If a `(:where ...)` clause exists and its predicate cannot be statically proven true (assume all are runtime checks for now), the CIF is marked as potentially performing `ConstructionFailure`. This information should be associated with the type/CIF and accessible by the type system. Pure CIFs have no `(:where)` or their constraints are trivially true.",
      "testStrategy": "Define types with and without `(:where)` clauses. Query the system (e.g., via a debug interface or by how the type checker treats calls) to verify that CIFs are correctly identified as pure or effectful (performing `ConstructionFailure`).",
      "priority": "medium",
      "dependencies": [
        8
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Data Structure for CIF Purity/Effect Status",
          "description": "Design and implement the data structure to represent whether a CIF (Constructed Immutable Form) is pure or potentially effectful (e.g., due to `ConstructionFailure`). This structure will be associated with type definitions.",
          "dependencies": [],
          "details": "Introduce an enum or a set of flags (e.g., `PurityStatus { PURE, POTENTIALLY_EFFECTFUL_CONSTRUCTION_FAILURE }`). This status should be attachable to the internal representation of a CIF, likely as a field in the `TypeDefinition` struct or a property in the symbol table entry for the type. Ensure it defaults to a sensible value (e.g., PURE or UNKNOWN) before analysis.",
          "status": "pending",
          "testStrategy": "Unit tests for the data structure itself, if it's more complex than a simple enum. Verify it can represent the required states. No functional tests involving CIF analysis yet."
        },
        {
          "id": 2,
          "title": "Implement `deftype` and `(:where)` Clause Detection in AST",
          "description": "Modify the compiler's Abstract Syntax Tree (AST) traversal mechanism (e.g., during `deftype` processing or a dedicated analysis pass) to identify `deftype` declarations and specifically locate any `(:where ...)` clauses within their definitions.",
          "dependencies": [],
          "details": "Extend the existing AST visitor or parser logic. When a `deftype` node is encountered, traverse its body to find any `(:where ...)` S-expression. The primary output of this step for a given `deftype` is the presence and content (or absence) of this clause. This subtask focuses only on detection, not analysis of the clause's content.",
          "status": "pending",
          "testStrategy": "Unit tests with various `deftype` AST snippets: some with `(:where)` clauses, some without, some with `(:where)` clauses in different positions or nested structures (if applicable to the language grammar) to ensure correct and robust detection. Verify that the detector correctly flags the presence/absence of `(:where)` and can extract the clause itself."
        },
        {
          "id": 3,
          "title": "Initial `(:where)` Clause Analysis for Effect Inference",
          "description": "Implement the initial, simplified logic to analyze `(:where ...)` clauses. For this initial version, any `deftype` containing one or more `(:where ...)` clauses will be considered potentially effectful, implying a potential `ConstructionFailure`. CIFs without `(:where)` clauses are considered pure.",
          "dependencies": [
            2
          ],
          "details": "Based on the detection from subtask 2: if one or more `(:where ...)` clauses are present in a `deftype` definition, the CIF is provisionally determined to have `POTENTIALLY_EFFECTFUL_CONSTRUCTION_FAILURE` status. If no `(:where)` clause is found, it's provisionally determined to be `PURE`. This step explicitly defers complex static analysis of the predicate within the `(:where)` clause, adhering to the 'assume all are runtime checks for now' constraint.",
          "status": "pending",
          "testStrategy": "Test with `deftype` examples. Input `deftype` ASTs (or relevant intermediate representations) to this analysis stage. If a `(:where)` clause was detected (by logic from subtask 2), the analysis output for the CIF should be 'effectful'. If absent, it should be 'pure'. This can be tested by checking an intermediate analysis result or log output."
        },
        {
          "id": 4,
          "title": "Store and Associate Inferred Purity/Effect Status with CIF Representation",
          "description": "Integrate the purity/effect status (determined in subtask 3) into the CIF's persistent representation using the data structure defined in subtask 1. This information must be reliably associated with the type definition for later use.",
          "dependencies": [
            1,
            3
          ],
          "details": "After analyzing a `deftype` (as per subtask 3), update the corresponding `TypeDefinition` object, symbol table entry, or other canonical representation of the CIF with the inferred `PurityStatus` (from subtask 1). Ensure this information is correctly stored and accessible throughout subsequent compilation phases where type information is used.",
          "status": "pending",
          "testStrategy": "After processing `deftype` definitions through the analysis pipeline (subtasks 2 and 3), inspect the internal representation of the types (e.g., via a debugger, dedicated test accessors, or by serializing type information). Verify that the correct `PurityStatus` has been stored for CIFs with and without `(:where)` clauses."
        },
        {
          "id": 5,
          "title": "Expose CIF Purity/Effect Status via Type System API",
          "description": "Provide a clear and well-defined API or mechanism for the type system and other compiler components (like the type checker or optimizer) to query the stored purity/effect status of any given CIF.",
          "dependencies": [
            4
          ],
          "details": "Add a method or accessor function to the type system's interface or to the type representation itself. For example, `TypeSystem.getCifPurity(typeId: TypeIdentifier) -> PurityStatus` or `TypeDefinition.getPurityStatus() -> PurityStatus`. This API will be the primary way other parts of the compiler access this inferred information.",
          "status": "pending",
          "testStrategy": "Unit tests for the new API. Create mock type definitions or use a test instance of the type system where CIFs have pre-set purity statuses (as if stored by subtask 4). Use the new API to retrieve these statuses and verify correctness. Test with types that have been analyzed and those that might not have (e.g., built-in types, if applicable, to ensure graceful handling)."
        }
      ]
    },
    {
      "id": 10,
      "title": "Dependent Types Implementation (Π-types and Σ-types)",
      "description": "Implement dependent function types (Π-types) and dependent pair/product types (Σ-types) in the type system and type checker.",
      "details": "Extend type representations: `PiType(var_name, var_type, body_type)`, `SigmaType(var_name, var_type, body_type)`. Update the type checker: For Π-types `(-> (x : A) (B x))`, when checking the function body `B x`, `x` is in scope with type `A`. For Σ-types `(Sigma (x : A) (B x))`, an instance is a pair `(a, b)` where `a : A` and `b : (B a)`. This requires the type checker to evaluate expressions at the type level (or use a distinct syntax for type-level computation if needed).",
      "testStrategy": "Define and type check functions using Π-types (e.g., a function returning a length-indexed vector `Vec A n`). Define and type check data structures using Σ-types (e.g., a pair where the type of the second element depends on the first `(Sigma (n : Nat) (Vec Bool n))`). Test with simple examples like `(deftype Vec ((A : Type) (n : Nat)) ...)` where `n` is used in constraints or structure.",
      "priority": "medium",
      "dependencies": [
        4
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define AST Nodes and Internal Representation for Π-types and Σ-types",
          "description": "Extend the Abstract Syntax Tree (AST) and internal type representation to include `PiType(var_name, var_type, body_type)` and `SigmaType(var_name, var_type, body_type)`. This forms the foundational data structures for dependent types.",
          "dependencies": [],
          "details": "Define `PiType` structure: `var_name` (String/Identifier), `var_type` (Type Expression), `body_type` (Type Expression which may refer to `var_name`). Define `SigmaType` structure: `var_name` (String/Identifier), `var_type` (Type Expression), `body_type` (Type Expression which may refer to `var_name`). Ensure these new type structures integrate with existing type representations (e.g., as variants in an enum or subclasses). Consider how `var_name` will be bound within `body_type` (e.g., de Bruijn indices or explicit name binding).",
          "status": "pending",
          "testStrategy": "Unit tests to verify the creation, structure, and properties (like free/bound variables) of `PiType` and `SigmaType` instances. Test serialization/deserialization if applicable."
        },
        {
          "id": 2,
          "title": "Implement Parser Extensions for Π-type and Σ-type Syntax",
          "description": "Update the language parser to recognize and correctly parse the syntax for Π-types (e.g., `(Pi (x : A) (B x))` or `(x : A) -> B x`) and Σ-types (e.g., `(Sigma (x : A) (B x))` or `(x : A) * B x`).",
          "dependencies": [
            1
          ],
          "details": "Define or confirm the concrete syntax for Π-types and Σ-types. Modify the parser grammar (e.g., LALR, PEG, or recursive descent) to incorporate these new syntactic forms. Ensure the parser constructs the correct `PiType` and `SigmaType` AST nodes defined in Subtask 1, capturing `var_name`, `var_type`, and `body_type` appropriately.",
          "status": "pending",
          "testStrategy": "Parser tests with various valid syntax examples for Π-types and Σ-types, including nested dependent types. Test invalid syntax to ensure proper error reporting. Verify correct AST node generation."
        },
        {
          "id": 3,
          "title": "Implement Core Type-Level Substitution and Normalization Engine",
          "description": "Develop the foundational engine for substituting values (or value expressions) into type expressions and normalizing these expressions. This is essential for resolving dependencies in Π-types and Σ-types during type checking.",
          "dependencies": [
            1
          ],
          "details": "Implement `substitute(type_expr, var_name, value_expr)`: Replaces free occurrences of `var_name` in `type_expr` with `value_expr`. Must correctly handle variable capture (e.g., via alpha-conversion or de Bruijn indices). Implement `normalize(type_expr)`: Reduces a type expression to its normal form. This may involve evaluating parts of expressions that are known at compile-time (e.g., constants, simple arithmetic if allowed in types, or reducing beta-redexes if functions are allowed in types). Ensure this engine can operate on all type structures, including the new `PiType` and `SigmaType` (e.g., substitution under binders).",
          "status": "pending",
          "testStrategy": "Unit tests for `substitute` with various scenarios: simple substitution, nested types, substitution under binders, and variable capture avoidance. Unit tests for `normalize` with expressions that should reduce to a canonical form and those already in normal form. Test with type expressions involving `PiType` and `SigmaType`."
        },
        {
          "id": 4,
          "title": "Implement Type Checking for Π-types (Dependent Functions)",
          "description": "Extend the type checker to handle formation, introduction (lambda abstraction), and elimination (application) rules for Π-types, utilizing the substitution and normalization engine.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Formation: For `(Pi (x : A) Bx)`, check that `A` is a valid type. Then, in a context extended with `x : A`, check that `Bx` is a valid type. Introduction (Lambda): For `lambda (x : A) . body`, type check `body` in a context extended with `x : A`. If `body : Bx`, the lambda has type `(Pi (x : A) Bx)`. Elimination (Application): For `f arg`, if `f : (Pi (x : A) Bx)` and `arg : A'`, check type equivalence of `A` and `A'`. The result type is `Bx` with `arg` substituted for `x`, followed by normalization: `normalize(substitute(Bx, x, arg))`. Update the typing environment/context management to handle the dependent variable `x` correctly.",
          "status": "pending",
          "testStrategy": "Type checker tests for: valid dependent function definitions and type inference; valid dependent function applications and correct result type computation (using substitution); type errors for ill-formed Π-types, incorrect lambda bodies, and mismatched function arguments. Test scoping of the dependent variable."
        },
        {
          "id": 5,
          "title": "Implement Type Checking for Σ-types (Dependent Pairs)",
          "description": "Extend the type checker for formation, introduction (pair construction), and elimination (projections) of Σ-types, utilizing the substitution and normalization engine.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Formation: For `(Sigma (x : A) Bx)`, check that `A` is a valid type. Then, in a context extended with `x : A`, check that `Bx` is a valid type. Introduction (Pair): For a pair `(a, b)`, type check `a` to get type `A`. Then, type check `b`. Its type must be equivalent to `Bx` with `a` substituted for `x`, followed by normalization: `normalize(substitute(Bx, x, a))`. The pair `(a,b)` then has type `(Sigma (x : A) Bx)`. Elimination (Projections): If `p : (Sigma (x : A) Bx)`, then `proj1(p) : A`. And `proj2(p) : normalize(substitute(Bx, x, proj1(p)))`. Implement syntax and type checking for pair construction and projections.",
          "status": "pending",
          "testStrategy": "Type checker tests for: valid dependent pair constructions and type inference; valid projections from dependent pairs and correct type computation for projections; type errors for ill-formed Σ-types, malformed pairs (e.g., second element's type doesn't match based on the first element's value). Test type equivalence involving substituted types."
        }
      ]
    },
    {
      "id": 11,
      "title": "Identity Types (`Id A x y`) and Path Equality (`refl`)",
      "description": "Implement Identity types `(Id A x y)` representing the proposition that `x` and `y` (of type `A`) are equal. Include the reflexivity constructor `refl`.",
      "details": "Introduce `Id` as a type constructor: `(Id A x y)` is a type. `A` is a type, `x` and `y` are terms of type `A`. Implement its constructor `refl : (-> (a : A) (Id A a a))`. The type checker must understand that `(Id A x y)` is a type and that `(refl z)` has type `(Id A z z)`. This is the first step towards path-based reasoning.",
      "testStrategy": "Define identity types for various base types `A`. Construct proofs of reflexivity, e.g., `(refl 5)` should have type `(Id Nat 5 5)`. Type check expressions involving `Id` types and `refl`. Test that `(Id Nat 1 2)` is a valid type, even if potentially uninhabited without further axioms.",
      "priority": "medium",
      "dependencies": [
        10
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define AST Node and Parser Rule for `Id` Type Constructor",
          "description": "Introduce the `Id` type constructor `(Id A x y)` into the system's syntax and Abstract Syntax Tree (AST). This allows the system to recognize and represent identity types.",
          "dependencies": [],
          "details": "Modify the lexer and parser to recognize the `Id` keyword and its three arguments: `A` (the type), `x` (the left-hand term), and `y` (the right-hand term).\nDefine a new AST node type (e.g., `IdTypeNode`) to represent `(Id A x y)`. This node should store references or sub-nodes for `A`, `x`, and `y`.\nEnsure the parser correctly constructs this AST node from input expressions like `(Id Nat zero zero)`.",
          "status": "pending",
          "testStrategy": "Write unit tests for the parser to verify that expressions of the form `(Id A x y)` are correctly parsed into the new `IdTypeNode` AST representation. Include tests for correct argument parsing and error handling for malformed `Id` expressions."
        },
        {
          "id": 2,
          "title": "Implement Type Checking Rule for `Id A x y`",
          "description": "Implement the type checking logic to validate that `(Id A x y)` is a well-formed type. This involves checking its components `A`, `x`, and `y`, and determining the universe of `(Id A x y)`.",
          "dependencies": [
            1
          ],
          "details": "In the type checker module, add a new rule or extend an existing one to handle `IdTypeNode`.\n1. Type check the `A` component. It must resolve to a type (e.g., `Type` or `Type_i` in a system with universes).\n2. Type check the `x` component. Let its inferred type be `A_x`.\n3. Type check the `y` component. Let its inferred type be `A_y`.\n4. Verify that `A_x` and `A_y` are judgmentally equal to `A` (the type specified in the `Id` expression).\n5. If all checks pass, the expression `(Id A x y)` itself is a type. It typically resides in the same universe as `A`, or a base universe like `Prop` if such a distinction exists (e.g., `Type` or `Prop`).",
          "status": "pending",
          "testStrategy": "Write unit tests for the type checker focusing on `Id` types.\n- Test valid cases: `(Id Nat zero zero)` should be recognized as a type (e.g., `Type`).\n- Test invalid cases: `(Id Nat (lambda (x:Nat) x) zero)` where the second argument is not of type `Nat` but `Nat -> Nat`.\n- Test invalid cases: `(Id zero zero zero)` where the first argument `zero` is not a type."
        },
        {
          "id": 3,
          "title": "Define AST Node and Parser Rule for `refl` Constructor",
          "description": "Introduce the `refl` constructor `(refl x)` into the system's syntax and AST. This allows the system to recognize and represent proofs of reflexivity.",
          "dependencies": [
            1
          ],
          "details": "Modify the lexer and parser to recognize the `refl` keyword and its single argument `x`.\nDefine a new AST node type (e.g., `ReflNode`) to represent `(refl x)`. This node should store a reference or sub-node for the term `x`.\nEnsure the parser correctly constructs this AST node from input expressions like `(refl zero)`.",
          "status": "pending",
          "testStrategy": "Write unit tests for the parser to verify that expressions of the form `(refl x)` are correctly parsed into the new `ReflNode` AST representation. Test with various term structures for `x`."
        },
        {
          "id": 4,
          "title": "Implement Type Checking Rule for `refl x`",
          "description": "Implement the type checking logic for the `refl` constructor. The term `(refl t)` should have the type `(Id A t t)`, where `A` is the type of `t`.",
          "dependencies": [
            2,
            3
          ],
          "details": "In the type checker module, add a new rule or extend an existing one to handle `ReflNode`.\n1. Type check the term `t` (from `(refl t)`). Let its inferred type be `A_inferred`.\n2. The type of the expression `(refl t)` is `(Id A_inferred t t)`.\n3. This involves constructing an `IdTypeNode` (or its internal representation) with `A = A_inferred`, left term = `t` (deep copy or reference), and right term = `t` (deep copy or reference).\nEnsure the type checker correctly forms this `Id` type as the result.",
          "status": "pending",
          "testStrategy": "Write unit tests for the type checker focusing on `refl` terms.\n- Test valid cases: `(refl zero)` should type check to `(Id Nat zero zero)` (assuming `zero : Nat`).\n- Test valid cases: `(refl true)` should type check to `(Id Bool true true)` (assuming `true : Bool`).\n- Verify that the components of the resulting `Id` type (`A`, `t`, `t`) are correctly set."
        },
        {
          "id": 5,
          "title": "Integrate `Id` and `refl` and Add End-to-End Tests",
          "description": "Ensure `Id` types and `refl` terms are fully integrated into the type system. Write comprehensive tests demonstrating their interaction and usage in definitions and proofs.",
          "dependencies": [
            4
          ],
          "details": "Verify that `refl` terms can be assigned to variables or used in contexts expecting an `Id` type with matching parameters.\nExample: `(define p : (Id Nat 5 5) (refl 5))` should type check.\nTest error reporting for type mismatches, e.g., trying to assign `(refl 5)` to `(Id Nat 5 6)` or `(Id Bool 5 5)`.\nConsider how `Id` types and `refl` interact with other language features like function definitions, let-bindings, and (if applicable) pattern matching or dependent elimination rules for `Id` (though elimination is not part of this task).\nCreate a set of small programs or definitions that use `Id` and `refl` to ensure they work correctly within the broader system.",
          "status": "pending",
          "testStrategy": "Develop end-to-end test files that include definitions and expressions using `Id` and `refl`.\n- Example 1: `(define reflexivity-of-zero : (Id Nat zero zero) (refl zero))`\n- Example 2: `(define generic-refl : (-> (A : Type) (x : A) (Id A x x)) (lambda (A_arg : Type) (lambda (x_arg : A_arg) (refl x_arg))))`\n- Test type inference where applicable.\n- Test error messages for incorrect usage in a larger context."
        }
      ]
    },
    {
      "id": 12,
      "title": "Effect Polymorphism for CIFs with Dependent Constraints",
      "description": "Enhance CIFs to support effect polymorphism based on dependent constraints, allowing them to be pure if a proof is available or effectful otherwise, as specified by `:constraint-proof` and `:performs-when-unproven`.",
      "details": "Extend `deftype` syntax: `(:constraint-proof (p : ProofType))` and `(:performs-when-unproven EffectToPerform)`. When a CIF is called: 1. Check if a proof for `ProofType` (often an `Id` type or a boolean proposition) is statically available (e.g., from type refinements of inputs, or given via `(:given proof-term)` at call site). 2. If proof is available, CIF is pure. 3. If proof is not available, CIF performs `EffectToPerform` (or `ConstructionFailure` if not specified). This requires the type system to handle proof terms and potentially rudimentary proof checking/inference. Example: `(deftype NonEmptyVec ((A : Type) (n : Nat)) (Array A) (:constraint-proof (p : (not (Id Nat n Nat.zero)))) (:performs-when-unproven ConstructionFailure))`.",
      "testStrategy": "Use the `NonEmptyVec` example from PRD. Test pure instantiation when `n` is statically non-zero (e.g., `(NonEmptyVec Int 1)`). Test effectful instantiation when `n` is a variable, requiring `try/catch`. Test providing an explicit proof via `(:given ...)`. Test with refined types like `Nat-Positive` that implicitly provide the proof, making the CIF pure.",
      "priority": "medium",
      "dependencies": [
        9,
        10,
        11
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend `deftype` Syntax and AST for Constraint Proofs and Effects",
          "description": "Modify the language parser and Abstract Syntax Tree (AST) to support the new `:constraint-proof` and `:performs-when-unproven` clauses within `deftype` definitions for Custom Inductive Families (CIFs).",
          "dependencies": [],
          "details": "Update the parser (e.g., LALR grammar, recursive descent parser) for `deftype` to recognize `(:constraint-proof (p : ProofType))` and `(:performs-when-unproven EffectToPerform)`. Extend the AST nodes for CIF definitions to store the proof variable name (e.g., `p`), the `ProofType` (as an AST representation of a type), and the `EffectToPerform` (as an identifier or a special marker like `ConstructionFailure`). Ensure robust error handling for malformed syntax.",
          "status": "pending",
          "testStrategy": "Unit tests for the parser with various valid and invalid `deftype` definitions using the new clauses. Verify correct AST structure and error reporting."
        },
        {
          "id": 2,
          "title": "Type System Representation for `ProofType` and Proof Terms",
          "description": "Enhance the type system to represent, type-check, and validate `ProofType` expressions (e.g., `(Id Nat n Nat.zero)`, boolean propositions) specified in CIFs, and to handle associated proof terms.",
          "dependencies": [
            1
          ],
          "details": "Define internal representations for `ProofType` within the type checker, potentially recognizing specific patterns like `Id` types or general propositional types. Implement type checking for `ProofType` expressions themselves to ensure they are well-formed. Establish how proof terms are represented (e.g., as specific AST nodes or by leveraging existing term representations). If `Id` types are central (e.g., `(Id T a b)`), ensure they can be robustly handled as propositions.",
          "status": "pending",
          "testStrategy": "Unit tests for type-checking various `ProofType` expressions (e.g., valid, invalid, involving `Id` types). Tests for representing and potentially type-checking basic proof terms against simple `ProofType`s."
        },
        {
          "id": 3,
          "title": "Implement Static Proof Derivation from Type Refinements",
          "description": "Develop logic within the type checker to statically infer the availability of a proof for a CIF's `ProofType` based on type refinements of its input arguments at the call site.",
          "dependencies": [
            2
          ],
          "details": "When a CIF is called, retrieve its declared `ProofType` (from AST, understood by type system via Subtask 2). Substitute CIF type parameters in `ProofType` with actual types/values from the call. Analyze the types of the provided arguments, looking for refinements (e.g., `x : Nat where x > 0`). Implement a mechanism (e.g., a small SMT query interface, a set of specialized inference rules) to determine if the available refinements logically entail the `ProofType`. For example, if `ProofType` is `(not (Id Nat n Nat.zero))` and an argument `n_val` for `n` has a refined type implying `n_val != 0`, the proof is considered available.",
          "status": "pending",
          "testStrategy": "Test with CIF calls where proofs can be derived from argument refinements (e.g., `n > 0` implies `n != 0`). Test cases where proofs cannot be derived. Verify correct interaction with the type refinement system."
        },
        {
          "id": 4,
          "title": "Implement Handling of Explicit `(:given proof-term)` at Call Sites",
          "description": "Extend CIF call-site syntax and type-checking to support explicitly provided proof terms via `(:given proof-term)`, and validate these terms against the CIF's `ProofType`.",
          "dependencies": [
            2
          ],
          "details": "Modify the parser for function/CIF calls to accept an optional `(:given proof-term)` clause. Store the provided `proof-term` (as an AST expression) in the call-site AST node. During type checking of the CIF call, if `(:given proof-term)` is present: type-check the `proof-term` itself. Verify that the type of the `proof-term` is compatible with or proves the CIF's `ProofType` (after substituting CIF parameters, using `ProofType` understanding from Subtask 2). This constitutes the 'rudimentary proof checking' for explicit proofs. If valid, the proof is considered available.",
          "status": "pending",
          "testStrategy": "Test CIF calls with valid and invalid `(:given proof-term)` clauses. Test type checking of various proof terms against different `ProofType`s. Ensure correct parsing of the new call-site syntax."
        },
        {
          "id": 5,
          "title": "Integrate Proof Availability into Effect Typing for CIF Calls",
          "description": "Modify the CIF call resolution and effect typing mechanism to use the determined proof availability (from implicit derivation or explicit provision) to assign either a 'pure' effect or the specified `EffectToPerform` (or default `ConstructionFailure`).",
          "dependencies": [
            3,
            4
          ],
          "details": "At each CIF call site, invoke the proof availability checks: first, the static derivation from refinements (Subtask 3), then the check for an explicit `(:given proof-term)` (Subtask 4). If a proof is deemed available through either mechanism: the CIF call is assigned a 'pure' effect in the effect system. If no proof is available: the CIF call is assigned the effect specified by `:performs-when-unproven` in its `deftype`. If `:performs-when-unproven` is not specified, assign a default `ConstructionFailure` effect (this effect must be defined and handled by the effect system, possibly as a compile-time error if unhandled or a specific runtime exception). Ensure the overall type and effect inference system correctly propagates these conditional effects.",
          "status": "pending",
          "testStrategy": "End-to-end tests for CIFs with various constraint proofs. Test calls where proof is available (implicitly/explicitly), verifying pure effect. Test calls where proof is unavailable, verifying the specified effect (or `ConstructionFailure`). Verify interaction with the broader effect system and error reporting for unproven constraints leading to `ConstructionFailure`."
        }
      ]
    },
    {
      "id": 13,
      "title": "Universe Hierarchy Implementation (Type, Type 1, ...)",
      "description": "Implement a basic universe hierarchy (e.g., `Type` or `Type 0`, `Type 1`, ...) to consistently handle types as first-class citizens and avoid paradoxes like Girard's paradox.",
      "details": "Introduce universe types: `Type_i` (or `Type i`). `Type_i` is itself of type `Type_{i+1}`. All user-defined types, Π-types, Σ-types, Id-types will reside in some universe `Type_i`. The type checker must enforce universe consistency rules (e.g., `(-> A B)` is in `Type_k` if `A : Type_i`, `B : Type_j` and `k = max(i,j)` for non-dependent functions, more complex for dependent). Start with a simple hierarchy like `Type` (for ordinary types) and `Kind` (for `Type` itself), or `Type 0`, `Type 1`.",
      "testStrategy": "Define types that take other types as parameters (e.g., `(List Type)`). Verify that the type checker correctly assigns universe levels and reports errors for universe inconsistencies (e.g., a type trying to contain itself in a way that violates hierarchy).",
      "priority": "medium",
      "dependencies": [
        10
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Define `Type_i` AST Node and Implement `Type_i : Type_{i+1}` Rule",
          "description": "Introduce an Abstract Syntax Tree (AST) node for universe types, denoted as `Type_i` (e.g., `Ast.Universe(level: int)`). Implement the fundamental typing rule where `Type_i` is itself of type `Type_{i+1}`. This involves representing universe levels and their hierarchical relationship.",
          "dependencies": [],
          "details": "Create a new AST variant, `UniverseNode(level: int)`, to represent `Type_i`. In the type system, this node itself represents a type. The core logic to be implemented is that `typeof(UniverseNode(i))` is `UniverseNode(i+1)`. This might require special handling in the type inference or type checking mechanism for these universe type constructors. Assume levels are non-negative integers. Consider how `Type_{i+1}` is represented, especially if `i` could reach a maximum representable level (though an unbounded conceptual model is fine for initial implementation).",
          "status": "pending",
          "testStrategy": "Unit tests: 1. Construct `Type 0` (e.g., `UniverseNode(0)`). Verify its inferred type is `Type 1` (e.g., `UniverseNode(1)`). 2. Construct `Type 1`, verify its type is `Type 2`. 3. Test the internal representation and equality of `UniverseNode` instances."
        },
        {
          "id": 2,
          "title": "Integrate `Type_i` into Core Type System and Basic Type Checking",
          "description": "Modify the language's core type representation to fully accommodate `Type_i` as a valid type. Update the type checker to recognize, infer types for, and check `Type_i` nodes, specifically enforcing the `Type_i : Type_{i+1}` rule during general type checking operations.",
          "dependencies": [
            1
          ],
          "details": "Ensure that `Type_i` (represented by `UniverseNode`) can appear in any context where a type is expected (e.g., type annotations, function signatures). The type checker's `infer_type(expression)` function should correctly return `UniverseNode(level+1)` when `expression` is `UniverseNode(level)`. The `check_type(expression, expected_type)` function must validate this relationship. This typically involves adding new cases to pattern matches over type structures within the type checker.",
          "status": "pending",
          "testStrategy": "Unit tests: 1. Type check an expression `Type 0` and assert its inferred type is `Type 1`. 2. Type check `Type 5` and assert its inferred type is `Type 6`. 3. Test type equality comparisons involving `Type_i` (e.g., `Type 0 == Type 0` is true, `Type 0 == Type 1` is false). 4. Test type checking an ill-typed universe, e.g. `Type 0 : Type 0` should fail."
        },
        {
          "id": 3,
          "title": "Assign Default Universe Levels to User-Defined and Primitive Types",
          "description": "Establish a default universe level (typically `Type 0`) for all newly defined data types (e.g., `struct Point`, `enum Color`) and any built-in primitive types (e.g., `Int`, `Bool`, `String`). Update the type definition processing and type checker to assign and verify these base universe levels.",
          "dependencies": [
            2
          ],
          "details": "When a user defines a new type (e.g., `data MyType = ...`), it should be assigned to `Type 0` by default, meaning `MyType : Type 0`. The type checker must be aware of this when `MyType` is used as a type. For example, in `let x : MyType = ...`, the type checker verifies that `MyType` is indeed a type (i.e., it resides in some `Type_i`). This subtask primarily concerns types that are not themselves parameterized by other types that might affect their universe level. If primitive types exist, they should also be assigned to `Type 0` (e.g. `Int : Type 0`).",
          "status": "pending",
          "testStrategy": "Unit tests: 1. Define a simple data type `Foo` and check that `typeof(Foo)` is `Type 0`. 2. If primitive types like `Integer` exist, check that `typeof(Integer)` is `Type 0`. 3. Test that using `Foo` as a type in a variable declaration (e.g., `let x : Foo = ...`) is valid and type checks correctly. 4. Test that a type definition itself is correctly recorded as being in `Type 0`."
        },
        {
          "id": 4,
          "title": "Implement Universe Level Calculation for Non-Dependent Function Types (Π-types)",
          "description": "Update the type checker to calculate and enforce the universe level for non-dependent function types (Π-types). The rule is: if `A -> B` is a function type, where `A : Type_i` and `B : Type_j`, then the type `(A -> B)` itself resides in `Type_{max(i,j)}`.",
          "dependencies": [
            3
          ],
          "details": "Modify the AST node for function types or the type checking logic for them. When type checking a function type constructor `T1 -> T2`: 1. Infer the types of `T1` and `T2`. These must be universe types, say `Type_i` and `Type_j` respectively (meaning `T1 : Type_i` and `T2 : Type_j`). 2. The type of the function type `(T1 -> T2)` is then `Type_{max(i,j)}`. This rule must be enforced. This applies when forming function types, e.g., `(Int -> Bool) : Type 0` because `Int : Type 0` and `Bool : Type 0`, so `max(0,0)=0`.",
          "status": "pending",
          "testStrategy": "Unit tests: 1. Given `A : Type 0`, `B : Type 0`, check `(A -> B) : Type 0`. 2. Given `A : Type 0`, `B : Type 1` (where `Type 1` itself is a type, e.g. `Type : Type 1`), check `(A -> B) : Type 1`. 3. Given `A : Type 1`, `B : Type 0`, check `(A -> B) : Type 1`. 4. Test nested function types, e.g., `(A -> (B -> C))`. 5. Test type checking failure if a component type (e.g., `A` or `B`) is not a valid type (i.e., not in any `Type_k`)."
        },
        {
          "id": 5,
          "title": "Extend Universe Rules to Dependent Π-types, Σ-types, and Id-types",
          "description": "Extend universe level calculation and checking to dependent function types (Π-types where codomain type depends on a term of the domain type). Also, define and implement rules for Σ-types (dependent pairs) and Id-types (identity/equality types) if they are part of the language, ensuring they reside in appropriate universes.",
          "dependencies": [
            4
          ],
          "details": "For dependent function types `(x:A) -> B(x)`: if `A : Type_i` and for any term `a:A`, the type `B(a) : Type_j`, then the type `((x:A) -> B(x))` resides in `Type_{max(i,j)}`. This requires the type checker to handle type dependencies. For Σ-types `(x:A) * B(x)`: if `A : Type_i` and `B(a) : Type_j` for `a:A`, then `((x:A) * B(x)) : Type_{max(i,j)}`. For Id-types `Id A x y`: if `A : Type_i` (and `x,y:A`), then `(Id A x y) : Type_i`. These rules ensure all major type formers are consistently placed within the universe hierarchy. The focus should be on implementing the rule for dependent Π-types first, then others as applicable.",
          "status": "pending",
          "testStrategy": "Unit tests: 1. For dependent Π-types: Define `A : Type 0`, and a type family `P : A -> Type 0`. Check `((x:A) -> P x) : Type 0`. 2. Define `A : Type 1`, `P : A -> Type 0`. Check `((x:A) -> P x) : Type 1`. 3. If Σ-types are implemented: `A : Type 0`, `B : A -> Type 0`. Check `((x:A) * B x) : Type 0`. 4. If Id-types are implemented: `A : Type 0`, `x:A`, `y:A`. Check `(Id A x y) : Type 0`. 5. Test cases that should fail due to universe inconsistencies in these dependent type formers."
        }
      ]
    },
    {
      "id": 14,
      "title": "Basic Module System (Import/Export, Namespacing)",
      "description": "Implement a basic module system for namespace management, allowing definitions to be organized into modules and selectively imported/exported.",
      "details": "Syntax for defining modules: `(module ModuleName (export symbol1 symbol2) body...)`. Syntax for importing: `(import ModuleName)` or `(import ModuleName (only symbol1) (prefix p-))`. The evaluator and type checker need to respect module boundaries and resolve symbols accordingly. Each module should have its own environment, linked to imported modules.",
      "testStrategy": "Create multiple module files. Define types and functions in one module, export some. Import them into another module and use them. Test for name collisions and how they are handled (e.g., qualified names or errors). Test different import syntaxes (full import, selective import, prefixed import).",
      "priority": "medium",
      "dependencies": [
        6
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend Parser for Module and Import Syntax",
          "description": "Modify the language parser to correctly recognize and parse `(module ModuleName (export ...) body...)` and `(import ModuleName ...)` expressions, generating corresponding Abstract Syntax Tree (AST) nodes.",
          "dependencies": [],
          "details": "Update the lexical analyzer (if necessary) and parser rules. Define new AST node types for `ModuleDefinition` (capturing module name, export list, and body expressions) and `ImportStatement` (capturing the module to import, an optional `only` list of symbols, and an optional `prefix` string). Ensure the parser can handle variations in export lists (e.g., empty, single, multiple symbols) and import options.",
          "status": "pending",
          "testStrategy": "Create unit tests for the parser with various valid module definitions and import statements, including different forms of export lists and import options (`only`, `prefix`). Test parsing of malformed module/import syntax to ensure proper error reporting and recovery if applicable."
        },
        {
          "id": 2,
          "title": "Implement Module Representation and Registry",
          "description": "Define data structures for representing modules, their environments, and exported symbols. Implement a global registry to manage and access defined modules.",
          "dependencies": [
            1
          ],
          "details": "Create a `Module` class or struct. This structure should contain: its name (string), a dedicated `Environment` instance (which can chain to a global environment or other imported module environments), a list or map of its exported symbols (mapping exported names to their actual values/definitions within the module), and the parsed AST of its body. Implement a `ModuleRegistry` (e.g., a singleton class or a global hash map) to store `Module` instances indexed by their names. This registry will be used to resolve module names during import and to prevent or manage redefinition of modules.",
          "status": "pending",
          "testStrategy": "Unit tests for `Module` creation, adding definitions to a module's internal environment, marking symbols as exported (storing necessary information to resolve them later), and retrieving modules by name from the `ModuleRegistry`. Test behavior for attempting to define duplicate module names."
        },
        {
          "id": 3,
          "title": "Process Module Definitions and Handle Exports",
          "description": "Implement the logic to process parsed `ModuleDefinition` AST nodes. This involves creating a new module instance, evaluating its body within its specific environment, and populating its export list based on the `(export ...)` clause.",
          "dependencies": [
            2
          ],
          "details": "When the evaluator encounters a `ModuleDefinition` AST node: \n1. Create a new `Module` instance using the parsed name and body.\n2. Register this new module in the `ModuleRegistry`.\n3. Create a new, isolated `Environment` for this module. This environment should be used for evaluating the module's body.\n4. Evaluate the expressions in the module's `body` within this new module-specific environment.\n5. After evaluation, for each symbol listed in the `(export ...)` clause, verify it's defined in the module's environment. If so, add it to the module's list/map of accessible exports (e.g., storing a mapping from the exported name to its evaluated value or definition within the module's environment).",
          "status": "pending",
          "testStrategy": "Test defining modules with various simple and complex bodies. Verify that only symbols specified in the `(export ...)` clause are marked as exported by the module object. Test error handling for exporting symbols that are not defined within the module's body. Ensure symbols defined within a module are encapsulated and not accessible globally unless explicitly exported and then imported."
        },
        {
          "id": 4,
          "title": "Implement Import Statement Processing",
          "description": "Implement the logic to process parsed `ImportStatement` AST nodes, making exported symbols from one module available in the current (importing) module's environment, respecting `(only ...)` and `(prefix ...)` clauses.",
          "dependencies": [
            3
          ],
          "details": "When the evaluator encounters an `ImportStatement` AST node within the context of a current (importing) module or top-level environment:\n1. Resolve the target module name specified in the import statement by looking it up in the `ModuleRegistry`. Handle errors if the module is not found.\n2. Retrieve the target module's list of exported symbols.\n3. For each exported symbol from the target module:\n    a. If an `(only symbol1 ...)` clause is present in the import statement, only consider symbols that are explicitly listed in this `only` clause.\n    b. Determine the name to be used for the symbol in the importing environment: if a `(prefix p- ...)` clause is present, prepend the specified prefix to the original symbol name. Otherwise, use the original symbol name.\n    c. Add a binding for this (potentially filtered and prefixed) name into the importing module's environment. This binding should effectively link to the original definition in the imported module's environment (e.g., by copying the value, creating an alias, or a thunk that resolves to the original).",
          "status": "pending",
          "testStrategy": "Test various import scenarios: importing all symbols from a module, importing a subset of symbols using `(only ...)` clause, importing symbols with a `(prefix ...)` clause, and combinations of `only` and `prefix`. Test error conditions such as importing from a non-existent module, attempting to import a non-exported symbol (if `only` specifies it), or name collisions if not handled by prefixing."
        },
        {
          "id": 5,
          "title": "Integrate Module System into Evaluator and Type Checker",
          "description": "Update the symbol resolution logic in the language's evaluator and type checker to be module-aware. This means respecting the current module's environment, its imports, and ensuring proper encapsulation.",
          "dependencies": [
            4
          ],
          "details": "Modify the core symbol lookup mechanisms:\n*   **Evaluator**: When resolving a symbol, the evaluator must first search the local environment of the current module. If the symbol is not found locally, it should then search through the symbols imported into the current module (respecting any prefixes or `only` restrictions applied during import). The evaluator needs to maintain a concept of the \"current module context\" during execution.\n*   **Type Checker**: Similarly, the type checker's symbol resolution for type lookups and variable type inference must follow the same module-aware path: local module environment first, then imported symbols. Type checking of a module's body should occur within its own isolated scope, considering its imports.\n*   Define how top-level definitions (code not explicitly within a `(module ...)` form) are handled. They might be placed in a default global module, or all code might be required to reside within modules.\n*   Ensure that the evaluation of a module body (during `(module ...)` processing) and import statements correctly sets and restores the current module context.",
          "status": "pending",
          "testStrategy": "Develop end-to-end tests. These tests should involve defining multiple modules, exporting and importing symbols between them using various options (`only`, `prefix`). Evaluate expressions and perform type checking on code that utilizes these cross-module symbols. Verify correct symbol resolution, proper type inference across module boundaries, and strict enforcement of encapsulation (e.g., inability to access non-exported symbols from another module directly)."
        }
      ]
    },
    {
      "id": 15,
      "title": "Hygienic Macro System (Initial Implementation)",
      "description": "Implement an initial version of a hygienic macro system, allowing users to define syntactic abstractions using `define-syntax` and syntax rules, following Scheme principles.",
      "details": "Implement `(define-syntax keyword (syntax-rules (literals...) ((pattern) template) ...))`. The macro expander should run after parsing but before type checking/evaluation. Focus on achieving hygiene to prevent accidental variable capture. This involves careful handling of symbols and their lexical contexts during macro expansion.",
      "testStrategy": "Define simple macros like `when`, `unless`, `let*` (if `let` is basic). Test macro expansion to ensure correct output AST. Crucially, test for hygiene: define macros that introduce bindings and use them in contexts where those binding names might clash with existing variables, verifying that the macro's bindings do not capture or shadow unintended variables.",
      "priority": "medium",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend Parser for `define-syntax` and `syntax-rules`",
          "description": "Modify the language's parser to recognize and correctly parse `define-syntax` forms. This includes parsing the `syntax-rules` structure, capturing the macro keyword, the list of literal identifiers, and the set of pattern-template rules.",
          "dependencies": [],
          "details": "Update the grammar to include `define-syntax` as a new top-level form or special form. The parser should produce an Abstract Syntax Tree (AST) node specifically representing a macro definition. This AST node must clearly delineate: 1. The keyword (symbol) being defined as a macro. 2. The list of literal symbols specified in `(literals...)`. 3. An ordered list of rules, where each rule is a pair of (pattern, template). Ensure the parser can handle nested structures within patterns and templates (e.g., nested lists, multiple pattern variables).",
          "status": "pending",
          "testStrategy": "Unit test the parser with various valid `define-syntax` expressions, including those with empty/non-empty literal lists, single/multiple rules, and complex nested patterns/templates. Also test with malformed `define-syntax` expressions to ensure proper error reporting."
        },
        {
          "id": 2,
          "title": "Implement Macro Definition Storage and Retrieval",
          "description": "Develop a system for storing parsed macro definitions and retrieving them when a potential macro invocation is encountered. This system will act as a compile-time macro environment.",
          "dependencies": [
            1
          ],
          "details": "Design a data structure (e.g., a hash map or a dedicated environment structure) to store macro definitions. This structure will map macro keywords (symbols) to their parsed `syntax-rules` objects (as produced by subtask 1). Implement functions to: 1. Add a new macro definition to this storage when a `define-syntax` form is processed. 2. Look up a macro definition by its keyword. This lookup will be used by the macro expander. The storage should be managed appropriately within the compilation phases.",
          "status": "pending",
          "testStrategy": "Unit test the storage mechanism by adding various macro definitions and then attempting to retrieve them. Test cases should include retrieving existing macros, attempting to retrieve non-existent macros (should return a clear 'not found' indication), and potentially re-defining macros (if allowed by the language semantics)."
        },
        {
          "id": 3,
          "title": "Implement `syntax-rules` Pattern Matching and Template Instantiation",
          "description": "Implement the core logic for `syntax-rules` expansion. This involves matching an input syntax form against the patterns of a macro's rules and, upon a successful match, instantiating the corresponding template.",
          "dependencies": [
            2
          ],
          "details": "Implement a pattern matcher that supports: \n- Identifiers as pattern variables (capturing parts of the input syntax).\n- Literal identifiers (as specified in the `literals...` list of `syntax-rules`), which must match exactly.\n- The underscore `_` as a wildcard pattern that matches anything without binding it.\n- The ellipsis `...` for matching zero or more occurrences of a preceding sub-pattern.\n- Nested list/vector patterns.\nImplement template instantiation logic that:\n- Substitutes pattern variables with the syntax they captured.\n- Handles ellipsis `...` in templates to correctly structure output based on sequence captures.\n- Reconstructs the output syntax (AST) based on the template. Initially, this can be non-hygienic, focusing on structural transformation.",
          "status": "pending",
          "testStrategy": "Unit test the pattern matcher and template instantiator with a variety of macro rules and input forms. Test cases should cover: simple matches, literal matching, wildcard usage, ellipsis matching (zero, one, multiple elements), nested patterns, and correct template instantiation with variable substitution and ellipsis expansion. Test error conditions like no matching rule."
        },
        {
          "id": 4,
          "title": "Implement Hygiene Mechanism for Macro Expansion",
          "description": "Introduce hygiene into the macro expansion process to prevent accidental variable capture. This ensures that identifiers maintain their correct lexical scope across macro expansion boundaries.",
          "dependencies": [
            3
          ],
          "details": "Choose and implement a hygiene algorithm (e.g., automatic renaming based on lexical context, or syntax objects carrying lexical information). \nKey aspects:\n1. When a macro expands, identifiers introduced by the macro's template (not originating from pattern variables) must be treated as if they were defined in the macro definition's lexical environment. This typically involves 'coloring' or 'tagging' symbols with their lexical context or renaming them to be unique.\n2. Identifiers captured by pattern variables from the macro call site must retain their original call-site lexical meaning.\n3. Literal identifiers in patterns should be compared based on their symbolic identity within their respective lexical contexts.\nThis subtask modifies the template instantiation part of subtask 3 to incorporate hygiene.",
          "status": "pending",
          "testStrategy": "Test with classic hygiene-testing macros. Examples:\n- A macro defining a local variable `x` and using it, ensuring it doesn't capture an `x` from the call site.\n- A macro using a free variable `y` (expected to be bound in the macro definition's scope), ensuring it doesn't accidentally pick up a `y` from the call site.\n- Macros that pass syntax containing free variables to other macros."
        },
        {
          "id": 5,
          "title": "Integrate Macro Expander into Compilation Pipeline",
          "description": "Integrate the hygienic macro expander into the overall compilation/evaluation pipeline. The expander should operate on the AST after parsing and before subsequent phases like type checking or evaluation.",
          "dependencies": [
            1,
            4
          ],
          "details": "Modify the compiler/interpreter's main workflow to include a macro expansion phase. This phase takes the AST produced by the parser (subtask 1) as input.\nImplement a traversal mechanism (e.g., a tree walk) over the AST. When a form that could be a macro call is encountered:\n1. Look up the identifier in the macro definition storage (from subtask 2).\n2. If it's a macro, invoke the hygienic expansion logic (from subtask 4).\n3. Replace the original macro call AST node with the expanded AST node.\n4. Recursively expand the resulting syntax, as macro expansions can produce new macro calls.\nThe expansion process continues until no more macro calls are present in the AST. The fully expanded AST is then passed to the next compilation stage (e.g., semantic analysis, type checking, or code generation/evaluation). Ensure `define-syntax` itself is handled correctly by this phase (i.e., it defines a macro rather than being expanded further).",
          "status": "pending",
          "testStrategy": "End-to-end tests using small programs that define and use one or more hygienic macros. Verify that the program's behavior after macro expansion is correct and that hygiene is maintained. Check interactions between multiple macros. Ensure the expander correctly processes top-level `define-syntax` forms and then uses those definitions for subsequent expansions."
        }
      ]
    },
    {
      "id": 16,
      "title": "Pattern Matching for Product and Sum Types",
      "description": "Implement a pattern matching system for destructuring product types (field access) and matching sum type variants, including syntax parsing, match expression evaluation, and type checker integration for exhaustiveness and type safety.",
      "details": "Develop a comprehensive pattern matching facility. This involves:\n1.  **Syntax Definition and Parsing**:\n    *   Define the S-expression syntax for `match` expressions, e.g., `(match <scrutinee-expr> (<pattern1> <body1>) ... (<patternN> <bodyN>))_`.\n    *   Define pattern syntax for: literals (e.g., `10`, `#t`, `\"hello\"`), variables (e.g., `x`), wildcard (`_`), product type destructuring (e.g., `(Point x y)`), and sum type variant matching (e.g., `(Some val)`, `(Left err)`).\n    *   Extend the existing S-expression parser (from Task 1) to produce AST nodes for `MatchExpression` and various `Pattern` types (e.g., `LiteralPattern`, `VariablePattern`, `WildcardPattern`, `ConstructorPattern`).\n2.  **Match Expression Evaluation**:\n    *   Implement the evaluation logic for `match` expressions.\n    *   The scrutinee expression is evaluated first.\n    *   Patterns are tried in the order they appear.\n    *   For the first matching pattern: \n        *   Bind any variables introduced by the pattern to the corresponding parts of the scrutinee's value.\n        *   Evaluate the associated body expression in an environment extended with these new bindings.\n    *   If no pattern matches, a runtime error should be raised (unless compile-time exhaustiveness checks prevent this scenario).\n3.  **Type Checker Integration**:\n    *   Extend the type checker (Task 4) to analyze `match` expressions.\n    *   **Type of Scrutinee**: The type of the scrutinee expression must be determined.\n    *   **Pattern Typing**: Each pattern must be type-compatible with the scrutinee's type. For constructor patterns, the constructor must belong to the scrutinee's type (if it's a sum type) or be the type itself (for product types).\n    *   **Variable Binding Types**: Infer types for variables bound within patterns. E.g., if matching `(Point x y)` against a `Point` type where `x` and `y` are `Nat`, then `x` and `y` are typed as `Nat` in the body.\n    *   **Body Typing**: Each body expression is type-checked in its extended environment. All bodies in a single `match` expression must have a common, compatible type, which becomes the type of the `match` expression itself.\n    *   **Exhaustiveness Checking**: The type checker must verify that the set of patterns for a given scrutinee type is exhaustive. \n        *   For sum types (defined via `deftype` from Task 5), all variants must be covered, or a wildcard pattern must be present.\n        *   For `Bool`, patterns for `#t` and `#f` (or a wildcard) are needed.\n        *   Report a compile-time error if a `match` expression is found to be non-exhaustive.\n    *   **Unreachability**: Optionally, warn about or disallow unreachable patterns (e.g., a pattern following a wildcard).\n4.  **Interaction with `deftype` (Task 5)**:\n    *   The pattern matching system must use the definitions of product and sum types created by `deftype`. This includes knowing the fields of product types and the variants of sum types to perform destructuring and exhaustiveness checks correctly.",
      "testStrategy": "1.  **Parser Tests**:\n    *   Verify correct parsing of `match` expressions with various patterns: literals, variables, wildcards, product constructors, sum variant constructors, nested patterns.\n    *   Test syntax errors for malformed `match` expressions or patterns.\n2.  **Evaluation Tests**:\n    *   Test successful matching and value destructuring for product types.\n    *   Test successful matching for different sum type variants.\n    *   Verify correct variable bindings from patterns are available in the body.\n    *   Confirm that the correct body is evaluated based on the first matching pattern.\n    *   Test wildcard pattern behavior.\n    *   Test runtime error for non-exhaustive matches if not caught by the type checker (e.g., if checking is disabled or for dynamic scenarios).\n3.  **Type Checking Tests**:\n    *   **Pattern Compatibility**: Test that patterns incompatible with the scrutinee's type are rejected (e.g., matching a `Nat` with a `Point` pattern).\n    *   **Variable Type Inference**: Verify types of variables bound in patterns are correctly inferred and used for type checking bodies.\n    *   **Body Type Consistency**: Ensure all bodies in a `match` expression conform to a common type. Test cases where body types differ and should result in an error.\n    *   **Exhaustiveness Checking**: \n        *   Sum types: Test with full coverage of variants, partial coverage (error), and coverage with a wildcard.\n        *   `Bool`: Test with `#t`, `#f` patterns; `#t` only (error); `#t`, `_` (ok).\n        *   Product types: Test destructuring patterns.\n    *   **Type Safety**: Ensure that using a bound variable with an incorrect type within a clause body results in a type error.\n4.  **Integration Tests with `deftype`**:\n    *   Define several product and sum types using `deftype`.\n    *   Write complex `match` expressions using these types, involving nested patterns and multiple clauses.\n    *   Verify correct evaluation, full type checking (including exhaustiveness), and error reporting.",
      "status": "pending",
      "dependencies": [
        4,
        5
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Define AST Nodes and Extend Parser for Match Expressions and Patterns",
          "description": "Define the Abstract Syntax Tree (AST) nodes for `MatchExpression` and various `Pattern` types (Literal, Variable, Wildcard, Constructor). Extend the existing S-expression parser (from Task 1) to parse the `(match ...)` syntax and pattern syntax into these AST nodes.",
          "dependencies": [],
          "details": "Define `MatchExpression` AST node: `(scrutinee: Expr, cases: List[(Pattern, Expr)])`. Define `Pattern` AST hierarchy: `LiteralPattern(value: LiteralValue)`, `VariablePattern(name: String)`, `WildcardPattern()`, `ConstructorPattern(constructorName: String, subPatterns: List[Pattern])`. The parser should handle S-expression syntax like `(match <scrutinee-expr> (<pattern1> <body1>) ... (<patternN> <bodyN>))` and patterns such as `10`, `#t`, `\"hello\"`, `x`, `_`, `(Point x y)`, `(Some val)`. Ensure the parser correctly integrates with existing AST structures and error reporting.",
          "status": "pending",
          "testStrategy": "Unit test the parser with various valid and invalid `match` expressions and pattern syntaxes. Verify correct AST node generation and appropriate error messages for syntax errors."
        },
        {
          "id": 2,
          "title": "Implement Core Match Expression Evaluation Logic",
          "description": "Implement the runtime evaluation mechanism for `MatchExpression` AST nodes. This includes evaluating the scrutinee, iterating through patterns in order, performing matching for literals, variables, and wildcards, binding variables to an extended environment, and executing the corresponding body. If no pattern matches, a runtime error should be raised.",
          "dependencies": [
            1
          ],
          "details": "Implement `eval_match(match_expr, env)`: 1. Evaluate `match_expr.scrutinee` in `env` to get `scrutinee_value`. 2. Iterate through `match_expr.cases`. For each `(pattern, body)`: a. Implement `match_pattern(pattern, scrutinee_value)` which returns `Option[Map[String, Value]]` representing new bindings. Initially, support `LiteralPattern` (compare value), `VariablePattern` (binds scrutinee_value to variable name), `WildcardPattern` (always matches, no bindings). b. If match succeeds, create `extended_env` from `env` plus new bindings. Evaluate `body` in `extended_env` and return result. 3. If loop finishes with no match, raise a runtime error (e.g., \"Non-exhaustive match error\"). `ConstructorPattern` matching logic will be fully developed in a later subtask (ID 5), but basic name matching could be stubbed if necessary.",
          "status": "pending",
          "testStrategy": "Test evaluation with `match` expressions involving literals, variables, and wildcards. Verify correct body execution, variable binding within the body's scope, and proper return values. Test non-matching scenarios to ensure runtime errors are raised."
        },
        {
          "id": 3,
          "title": "Integrate Match Expressions into Type Checker: Scrutinee, Basic Pattern, and Body Typing",
          "description": "Extend the type checker (Task 4) to analyze `MatchExpression` AST nodes. This involves: determining the scrutinee's type; type checking basic patterns (Literal, Variable, Wildcard) against the scrutinee's type; inferring types for variables bound in these simple patterns; and ensuring all bodies in a single `match` expression have a common, compatible type, which becomes the type of the `match` expression.",
          "dependencies": [
            1
          ],
          "details": "Implement `typecheck_match(match_expr, type_env)`: 1. Typecheck `match_expr.scrutinee` to get `scrutinee_type`. 2. For each `(pattern, body)`: a. Implement `typecheck_pattern(pattern, scrutinee_type, type_env)` for basic patterns: `LiteralPattern` (literal's type must be compatible with `scrutinee_type`), `VariablePattern` (variable is typed as `scrutinee_type`; add to `extended_type_env`), `WildcardPattern` (compatible with any type, no new bindings). b. Typecheck `body` in the `extended_type_env` (derived from `type_env` plus pattern bindings) to get `body_type`. 3. Collect all `body_type`s. Compute their least common supertype (LUB). This LUB is the type of the `match` expression. If no LUB exists (incompatible body types), report a type error. `ConstructorPattern` type checking will be fully developed in a later subtask (ID 5).",
          "status": "pending",
          "testStrategy": "Unit test type checking for `match` expressions with simple patterns. Verify correct type inference for the overall expression and for variables bound by patterns. Test type mismatches (e.g., literal pattern vs. scrutinee type, incompatible body types) to ensure errors are reported."
        },
        {
          "id": 4,
          "title": "Implement Exhaustiveness and Unreachability Checks in Type Checker",
          "description": "Enhance the type checker to perform exhaustiveness checks for `match` expressions, reporting a compile-time error if a match is not exhaustive. This primarily applies to sum types (defined via `deftype` from Task 5) and booleans. Optionally, implement unreachability checks for patterns (e.g., a pattern following a wildcard).",
          "dependencies": [
            3
          ],
          "details": "Extend `typecheck_match` or add a separate pass: 1. **Exhaustiveness Checking**: `check_exhaustiveness(patterns, scrutinee_type, type_definitions_from_Task5)`: a. If `scrutinee_type` is `Bool`, check if patterns cover `#t` and `#f`, or if a wildcard `_` is present. b. If `scrutinee_type` is a sum type, retrieve its variants from `type_definitions_from_Task5`. Check if all variants are covered by `ConstructorPattern`s or if a wildcard `_` is present. c. If not exhaustive, report a compile-time error. 2. **Unreachability Checking (Optional)**: `check_unreachability(patterns)`: Analyze patterns in sequence. If a pattern `P_i` is found to be unreachable because preceding patterns `P_1 ... P_{i-1}` cover all cases `P_i` could match, issue a warning or compile-time error. For example, any pattern after `_` or `x` (variable pattern of the same type) is unreachable.",
          "status": "pending",
          "testStrategy": "Test exhaustiveness checking with various sum types (from `deftype`) and boolean scrutinees. Verify that non-exhaustive matches are caught at compile time. Test cases with and without wildcard patterns. If implementing unreachability, test scenarios with redundant or clearly unreachable patterns."
        },
        {
          "id": 5,
          "title": "Implement Constructor Pattern Destructuring and Full Type System Integration",
          "description": "Fully implement pattern matching for product type destructuring (e.g., `(Point x y)`) and sum type variant matching (e.g., `(Some val)`). This involves refining both evaluation (`match_pattern`) and type checking (`typecheck_pattern`) to correctly handle `ConstructorPattern`s, using type definitions from `deftype` (Task 5) for field/variant names, arity, and types of sub-patterns/payloads.",
          "dependencies": [
            2,
            4
          ],
          "details": "1. **Refine Evaluation (`match_pattern` for `ConstructorPattern`)**: a. For product types: Check constructor name matches type name, arity matches field count. Recursively match sub-patterns against field values. b. For sum types: Check constructor name matches variant tag, arity matches variant payload. Recursively match sub-patterns against payload values. Aggregate bindings from sub-matches. 2. **Refine Type Checking (`typecheck_pattern` for `ConstructorPattern`)**: a. Use `type_definitions_from_Task5` to get info about product type fields or sum type variants. b. For product types: Verify constructor name is the type name, arity matches. For each sub-pattern and corresponding field type, recursively call `typecheck_pattern` and add inferred bindings to `extended_type_env`. c. For sum types: Verify constructor name is a valid variant of `scrutinee_type`, arity matches. For each sub-pattern and corresponding variant parameter type, recursively call `typecheck_pattern` and add inferred bindings. This ensures variables bound within constructor patterns (e.g., `x`, `y` in `(Point x y)`) receive their correct types for use in the body.",
          "status": "pending",
          "testStrategy": "Test with various product and sum types defined via `deftype`. Verify correct destructuring of values and variable binding during evaluation. Verify correct type inference for variables bound within constructor patterns. Test error conditions like arity mismatches, incorrect constructor names, and type errors within sub-patterns."
        }
      ]
    },
    {
      "id": 17,
      "title": "Implement Core Standard Library (Nat, Bool, Option, List)",
      "description": "Define fundamental types Nat, Bool, Option, and List using `deftype`, implement their Canonical Instantiation Functions (CIFs), and provide a set of basic operations and functions for each. This establishes essential building blocks for practical programming in PathFinder LISP.",
      "details": "This task involves defining the core data types for the PathFinder LISP standard library. These types will be foundational for most programs written in the language.\n\n**General Requirements:**\n- All specified types (Nat, Bool, Option T, List T) must be defined using the `deftype` mechanism (from Task 5).\n- Canonical Instantiation Functions (CIFs) for each type and its variants (e.g., `Some` for `Option`, `Cons` for `List`) should be available as per the `deftype` specification.\n- Operations and functions for these types should be implemented as PathFinder LISP functions and must be type-checked by the system's type checker (from Task 4).\n- These `deftype` definitions for `Nat` and `Bool` will serve as the canonical representations in the standard library, potentially building upon or replacing any primitive notions from Task 3 to ensure they integrate seamlessly with user-defined types and the `deftype` system.\n- The `deftype` mechanism must support parameterized types (e.g., `Option T`, `List T`) and recursive type definitions (e.g., `List T`, Peano `Nat`). This might be a requirement for Task 5 or an aspect to be confirmed during this task's implementation.\n\n**Type Definitions and Operations:**\n\n1.  **Nat (Natural Numbers):**\n    *   Definition: `(deftype Nat (Zero) (Succ Nat))` (Peano representation).\n    *   CIFs: `Zero`, `Succ`.\n    *   Operations:\n        *   `isZero?: Nat -> Bool`\n        *   `add: Nat Nat -> Nat`\n        *   `subtract: Nat Nat -> Option Nat` (Subtraction is partial)\n        *   `equals?: Nat Nat -> Bool`\n        *   `lessThan?: Nat Nat -> Bool`\n\n2.  **Bool (Booleans):**\n    *   Definition: `(deftype Bool (PFalse) (PTrue))` (Using PFalse/PTrue to avoid conflict with potential #f/#t literals if they are different entities, or simply `False`/`True` if the naming convention allows).\n    *   CIFs: `PFalse`, `PTrue` (or `False`, `True`).\n    *   Operations:\n        *   `and: Bool Bool -> Bool`\n        *   `or: Bool Bool -> Bool`\n        *   `not: Bool -> Bool`\n\n3.  **Option T (Optional Values):**\n    *   Definition: `(deftype (Option T) (None) (Some T))` (Parameterized sum type).\n    *   CIFs: `None`, `Some`.\n    *   Operations:\n        *   `isSome?: (Option T) -> Bool`\n        *   `isNone?: (Option T) -> Bool`\n        *   `getOrElse: (Option T) T -> T` (Requires default value of type T)\n        *   `mapOption: (Option T) (T -> U) -> (Option U)`\n\n4.  **List T (Lists):**\n    *   Definition: `(deftype (List T) (Nil) (Cons T (List T)))` (Parameterized, recursive sum type).\n    *   CIFs: `Nil`, `Cons`.\n    *   Operations:\n        *   `isEmpty?: (List T) -> Bool`\n        *   `head: (List T) -> (Option T)`\n        *   `tail: (List T) -> (Option (List T))`\n        *   `append: (List T) (List T) -> (List T)`\n        *   `length: (List T) -> Nat`\n        *   `mapList: (List T) (T -> U) -> (List U)`\n        *   `filterList: (List T) (T -> Bool) -> (List T)`\n        *   `foldLeft: (List T) U ((U T) -> U) -> U` (Initial accumulator of type U, function takes accumulator and list element, returns new accumulator)\n        *   `reverse: (List T) -> (List T)`\n\n**Implementation Considerations:**\n-   The definitions should be placed in a standard library module that can be automatically loaded or easily imported.\n-   Ensure error handling for partial functions (e.g., `head` on an empty list should return `None`).\n-   Performance considerations for list operations, especially for larger lists (though initial implementations can be straightforward recursive versions).",
      "testStrategy": "Testing will involve unit tests for each type and its operations, focusing on CIFs, functional correctness, type checking, and edge cases.\n\n**1. Nat Tests:**\n    - Verify `(Zero)` and `(Succ ...)` create `Nat` values.\n    - Test `isZero?` on `Zero` and non-zero numbers.\n    - Test `add` with various combinations (e.g., `(add Zero Zero)`, `(add Zero N)`, `(add N Zero)`, `(add N M)`).\n    - Test `subtract` (e.g., `(subtract N M)` where N > M, N = M, N < M returning `None`).\n    - Test `equals?` and `lessThan?` with various pairs.\n    - Type checking: `(add Zero PTrue)` should be a type error.\n\n**2. Bool Tests:**\n    - Verify `(PFalse)` and `(PTrue)` create `Bool` values.\n    - Test `and`, `or`, `not` with all input combinations.\n    - Type checking: `(and PTrue Zero)` should be a type error.\n\n**3. Option T Tests:**\n    - Verify `(None)` and `(Some val)` (e.g., `(Some 10)`, `(Some PTrue)`) create `Option Nat` and `Option Bool` respectively.\n    - Test `isSome?` and `isNone?` on `(None)` and `(Some val)`.\n    - Test `getOrElse` for both `None` and `Some` cases.\n    - Test `mapOption` with functions transforming the value, and with `None`.\n    - Type checking: `(getOrElse (Some 10) PFalse)` should be a type error if types don't match. `mapOption`'s function argument and return types must be checked.\n\n**4. List T Tests:**\n    - Verify `(Nil)` and `(Cons val list)` create `List` values (e.g., `(List Nat)`).\n    - Test `isEmpty?` on `(Nil)` and non-empty lists.\n    - Test `head` and `tail` on empty and non-empty lists (expecting `Option` results).\n    - Test `append` with empty lists and non-empty lists.\n    - Test `length` for various list sizes.\n    - Test `mapList`, `filterList`, `foldLeft`, `reverse` with various inputs: empty lists, single-element lists, multiple-element lists, and functions with different behaviors.\n    - Type checking: `(Cons 1 (Cons PTrue (Nil)))` should be a type error for a homogeneous list `(List Nat)`. Ensure higher-order functions' arguments are correctly type-checked.\n\n**5. Integration Tests:**\n    - Write small programs combining these types, e.g., a function that processes a `(List (Option Nat))`.\n    - Ensure CIFs are correctly evaluated by the evaluation engine.\n    - Verify that all defined functions and operations are correctly type-checked by the type system (Task 4) when used in various expressions.",
      "status": "pending",
      "dependencies": [
        4,
        5
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Define `Bool` Type and Implement Core Boolean Operations",
          "description": "Defines the `Bool` type using `deftype` with `PFalse` and `PTrue` variants. Implements its Canonical Instantiation Functions (CIFs) and the fundamental boolean operations: `and`, `or`, `not`. This subtask establishes the foundational boolean logic for the standard library.",
          "dependencies": [],
          "details": "1. Define `Bool` using `(deftype Bool (PFalse) (PTrue))`.\n2. Ensure `deftype` (from Task 5) generates `PFalse` and `PTrue` as CIFs.\n3. Implement PathFinder LISP functions for:\n   - `and: Bool Bool -> Bool`\n   - `or: Bool Bool -> Bool`\n   - `not: Bool -> Bool`\n4. Ensure all functions are type-checked by the system's type checker (from Task 4).\n5. Place these definitions in the standard library module for automatic loading or easy import.",
          "status": "pending",
          "testStrategy": "Unit test `PFalse` and `PTrue` CIFs. Test `and`, `or`, `not` operations with all possible boolean input combinations (e.g., `(and PTrue PFalse)`, `(or PFalse PFalse)`, `(not PTrue)`) to verify correctness."
        },
        {
          "id": 2,
          "title": "Define `Nat` Type and Implement Basic Query and Comparison Operations",
          "description": "Defines the `Nat` (Natural Numbers) type using Peano representation `(deftype Nat (Zero) (Succ Nat))` via `deftype`. Implements its CIFs (`Zero`, `Succ`) and basic operations that query state or compare `Nat` values, returning `Bool`: `isZero?`, `equals?`, `lessThan?`. This subtask relies on `deftype` supporting recursive type definitions.",
          "dependencies": [
            1
          ],
          "details": "1. Define `Nat` using `(deftype Nat (Zero) (Succ Nat))`. Confirm `deftype` (Task 5) supports recursive definitions as this is a recursive type.\n2. Ensure `Zero` (CIF for the `Zero` variant) and `Succ` (CIF for the `Succ` variant, taking a `Nat`) are available as per `deftype` specification.\n3. Implement PathFinder LISP functions for:\n   - `isZero?: Nat -> Bool`\n   - `equals?: Nat Nat -> Bool` (e.g., implement recursively by comparing structures: `(equals? Zero Zero)` is `PTrue`, `(equals? (Succ m) (Succ n))` is `(equals? m n)`, other cases `PFalse`)\n   - `lessThan?: Nat Nat -> Bool` (e.g., implement recursively: `(lessThan? Zero (Succ n))` is `PTrue`, `(lessThan? (Succ m) (Succ n))` is `(lessThan? m n)`, other cases `PFalse`)\n4. These functions will use the `Bool` type and its CIFs (`PTrue`, `PFalse`) from subtask 1 for their return values.\n5. Ensure all functions are type-checked. Place these definitions in the standard library module.",
          "status": "pending",
          "testStrategy": "Unit test `Zero` and `Succ` CIFs (e.g., `(Zero)` creates a Nat, `(Succ (Zero))` creates a Nat). Test `isZero?` on `Zero` and non-Zero `Nat`s (e.g., `(Succ Zero)`). Test `equals?` with identical and different `Nat`s. Test `lessThan?` with various pairs of `Nat`s (e.g., `(lessThan? Zero (Succ Zero))`, `(lessThan? (Succ Zero) Zero)`)."
        },
        {
          "id": 3,
          "title": "Define Parameterized `Option T` Type and Implement Core Operations",
          "description": "Defines the parameterized `Option T` type using `deftype` with `None` and `Some T` variants: `(deftype (Option T) (None) (Some T))`. Implements its CIFs (`None`, `Some`) and core operations: `isSome?`, `isNone?`, `getOrElse`, `mapOption`. This subtask relies on `deftype` supporting parameterized types.",
          "dependencies": [
            1
          ],
          "details": "1. Define `Option T` using `(deftype (Option T) (None) (Some T))`. Confirm `deftype` (Task 5) supports parameterized types.\n2. Ensure `None` (CIF for the `None` variant, type `(Option T)` for any `T`) and `Some` (CIF for the `Some` variant, type `T -> (Option T)`) are available.\n3. Implement PathFinder LISP functions for:\n   - `isSome?: (Option T) -> Bool`\n   - `isNone?: (Option T) -> Bool`\n   - `getOrElse: (Option T) T -> T` (takes a default value of type T)\n   - `mapOption: (Option T) (T -> U) -> (Option U)` (a higher-order function)\n4. `isSome?` and `isNone?` will use the `Bool` type from subtask 1.\n5. Ensure all functions are type-checked, including correct handling of generic types `T` and `U`. Place in standard library module.",
          "status": "pending",
          "testStrategy": "Unit test `None` and `Some` CIFs (e.g., `(None)` as `(Option Nat)`, `(Some (Zero))` as `(Option Nat)`). Test `isSome?`/`isNone?` on both `None` and `(Some value)` instances. Test `getOrElse` for both `None` (should return the default value) and `(Some value)` (should return the inner value). Test `mapOption` with functions that change type and/or value, applied to both `None` and `(Some value)`."
        },
        {
          "id": 4,
          "title": "Implement `Nat` Arithmetic Operations: `add` and `subtract`",
          "description": "Implements the arithmetic operations `add: Nat Nat -> Nat` and `subtract: Nat Nat -> Option Nat` for the `Nat` type defined in subtask 2. The `subtract` operation will utilize the `Option Nat` type (from subtask 3) to correctly handle cases where subtraction would result in a negative number (which is not representable by `Nat`).",
          "dependencies": [
            2,
            3
          ],
          "details": "1. Implement PathFinder LISP functions for:\n   - `add: Nat Nat -> Nat` (Implement recursively: `(add Zero n) = n`, `(add (Succ m) n) = (Succ (add m n))`)\n   - `subtract: Nat Nat -> Option Nat` (Implement recursively: `(subtract m Zero) = (Some m)`, `(subtract Zero (Succ n)) = (None)`, `(subtract (Succ m) (Succ n)) = (subtract m n)`)\n2. These functions operate on the `Nat` type (defined in subtask 2) and `subtract` returns an `(Option Nat)` (defined in subtask 3).\n3. Ensure all functions are type-checked. Place in standard library module.",
          "status": "pending",
          "testStrategy": "Unit test `add` with various `Nat` inputs, including `Zero` as one or both arguments (e.g., `(add (Succ (Succ Zero)) (Succ Zero))`). Test `subtract` for cases yielding `(Some Nat)` (e.g., 5-3, 3-0, 3-3, where Nats are Peano numbers) and cases yielding `None` (e.g., 3-5, 0-3)."
        },
        {
          "id": 5,
          "title": "Define Parameterized `List T` Type and Implement All Associated Operations",
          "description": "Defines the parameterized, recursive `List T` type using `deftype`: `(deftype (List T) (Nil) (Cons T (List T)))`. Implements its CIFs (`Nil`, `Cons`) and a comprehensive set of list operations as specified. This subtask relies on `deftype` supporting both parameterized and recursive types.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "1. Define `List T` using `(deftype (List T) (Nil) (Cons T (List T)))`. Confirm `deftype` (Task 5) supports parameterized and recursive types.\n2. Ensure `Nil` (CIF for `Nil` variant, type `(List T)` for any `T`) and `Cons` (CIF for `Cons` variant, type `T (List T) -> (List T)`) are available.\n3. Implement PathFinder LISP functions for:\n   - `isEmpty?: (List T) -> Bool` (uses `Bool` from subtask 1)\n   - `head: (List T) -> (Option T)` (uses `Option T` from subtask 3; returns `None` for an empty list)\n   - `tail: (List T) -> (Option (List T))` (uses `Option (List T)` from subtask 3; returns `None` for an empty list)\n   - `append: (List T) (List T) -> (List T)`\n   - `length: (List T) -> Nat` (uses `Nat` from subtask 2, specifically `Zero` and `Succ` for a recursive definition: `(length Nil) = Zero`, `(length (Cons _ t)) = (Succ (length t))`)\n   - `mapList: (List T) (T -> U) -> (List U)`\n   - `filterList: (List T) (T -> Bool) -> (List T)` (predicate function uses `Bool` from subtask 1)\n   - `foldLeft: (List T) U ((U T) -> U) -> U`\n   - `reverse: (List T) -> (List T)` (can be implemented efficiently using `foldLeft`)\n4. Ensure all functions are type-checked, including generic types `T` and `U`. Place in standard library module. Implement functions recursively where appropriate. Initial implementations should focus on correctness; performance can be addressed later if needed.",
          "status": "pending",
          "testStrategy": "Unit test `Nil` and `Cons` CIFs. Test `isEmpty?` on `Nil` and non-empty lists. Test `head`/`tail` on empty lists (expect `None` variant of `Option`) and non-empty lists. Test `append` with various list combinations (empty/non-empty with empty/non-empty). Test `length` on empty and non-empty lists. Test `mapList` with identity and transforming functions. Test `filterList` with predicates that select all, none, or some elements. Test `foldLeft` for common use cases like sum, product, and list construction (e.g., implementing reverse). Test `reverse` independently."
        }
      ]
    },
    {
      "id": 18,
      "title": "Implement Constraint System for Type Definitions and CIFs",
      "description": "Extend the type system to support constraints on type definitions using a `:where` clause. This includes parsing constraint expressions, implementing validation logic for both compile-time and runtime checks, and integrating these checks into Canonical Instantiation Functions (CIFs).",
      "details": "**1. Parser Extension (extending Task 5 `deftype`):**\n   - Modify the parser to recognize an optional `:where` clause in `deftype` expressions.\n   - Syntax: `(deftype TypeName ((field1 Type1) (field2 Type2) ...) :where <constraint-expression>)`\n   - The `<constraint-expression>` is a single expression that should evaluate to a boolean. It will be parsed into an AST node.\n   - Ensure field names (e.g., `field1`, `field2`) are correctly scoped and accessible within the `<constraint-expression>`.\n\n**2. AST Representation:**\n   - Define how the parsed constraint expression is stored within the AST representation of the type definition (e.g., as an additional child node or property of the type definition AST node).\n\n**3. Type Checker Integration (interacting with Task 4):**\n   - **Constraint Expression Type Checking:** The type checker must verify that the `<constraint-expression>` evaluates to a Boolean type. An error should be reported if it does not.\n   - **Compile-Time Constraint Validation:**\n     - Implement logic to attempt static evaluation of constraints if they involve only literal values or other compile-time evaluable expressions.\n     - If a constraint is proven true at compile-time, it can be noted or optimized.\n     - If a constraint is proven false at compile-time, the type definition itself should be considered invalid, and a type error reported.\n\n**4. CIF Modification (extending Task 5 CIFs):**\n   - Modify the generation and/or evaluation logic of Canonical Instantiation Functions (CIFs) for types that have constraints.\n   - **Runtime Constraint Evaluation:** For constraints not fully resolved at compile-time, the CIF must evaluate the constraint expression after the basic instance of the type is created but before it's returned.\n   - The evaluation environment for the constraint expression within the CIF must bind the type's field names to the corresponding values from the instance being created.\n   - If a runtime constraint evaluates to `#f` (false), the CIF must raise a distinct runtime error (e.g., `ConstraintViolationError`), preventing the instantiation of the type.\n\n**5. Evaluation of Constraints:**\n   - Constraint expressions will be evaluated using the existing evaluation engine (developed in Task 2).\n   - Ensure the evaluation context for constraints correctly resolves field names and any functions or values available in the scope where the type is defined or instantiated.\n\n**6. Error Reporting:**\n   - Implement clear and informative error messages for:\n     - Syntax errors in `:where` clauses.\n     - Type errors in constraint expressions (e.g., expression not Boolean-typed).\n     - Compile-time constraint violations (e.g., constraint statically proven false).\n     - Runtime constraint violations, indicating the specific type, the values being instantiated, and the constraint expression that failed.",
      "testStrategy": "**1. Parser Tests:**\n   - Test `deftype` with valid `:where` clauses using various expressions (e.g., `(> field1 0)`, `(and (> field1 0) (< field2 100))`, `(string=? (kind-of-shape) \"circle\")`).\n   - Test `deftype` with syntactically invalid `:where` clauses.\n   - Test `deftype` without a `:where` clause to ensure backward compatibility.\n\n**2. Type Checker & Compile-Time Constraint Tests:**\n   - Define a type with a constraint that is statically true (e.g., `(deftype T1 (x Nat) :where (> 5 0))`). Verify the type definition is accepted.\n   - Define a type with a constraint that is statically false (e.g., `(deftype T2 (x Nat) :where (< 0 0))`). Verify the type definition is rejected with a compile-time error.\n   - Define a type with a constraint expression that does not evaluate to Boolean (e.g., `(deftype T3 (x Nat) :where x)`). Verify a type error is reported.\n   - Verify correct scoping of field names within constraint expressions during type checking.\n\n**3. CIF & Runtime Constraint Tests:**\n   - Define `(deftype PositiveNat (value Nat) :where (> value 0))`:\n     - Test `(PositiveNat 10)`: Should succeed and return an instance.\n     - Test `(PositiveNat 0)`: Should raise a runtime `ConstraintViolationError`.\n   - Define `(deftype LimitedRange (val Int) :where (and (>= val 0) (<= val 10)))`:\n     - Test `(LimitedRange 5)`: Should succeed.\n     - Test `(LimitedRange -1)`: Should raise a runtime `ConstraintViolationError`.\n     - Test `(LimitedRange 11)`: Should raise a runtime `ConstraintViolationError`.\n   - Define `(deftype OrderedPair (x Nat) (y Nat) :where (< x y))`:\n     - Test `(OrderedPair 1 2)`: Should succeed.\n     - Test `(OrderedPair 2 1)`: Should raise a runtime `ConstraintViolationError`.\n\n**4. Error Reporting Tests:**\n   - For each failure mode (syntax, type error, compile-time violation, runtime violation), verify that the error message is clear, indicates the source of the error, and provides relevant context (e.g., failed constraint expression, values involved).",
      "status": "pending",
      "dependencies": [
        5,
        4
      ],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend Parser and AST for `:where` Constraint Clause",
          "description": "Modify the `deftype` parser to recognize an optional `:where` clause and define the Abstract Syntax Tree (AST) structure for storing the parsed constraint expression. This forms the foundation for constraint processing.",
          "dependencies": [],
          "details": "1. Update the parser grammar for `deftype` expressions to support the syntax: `(deftype TypeName ((field1 Type1) ...) :where <constraint-expression>)`.\n2. The `<constraint-expression>` should be parsed into a standard AST node.\n3. Define how this constraint AST node is stored within the AST representation of the type definition (e.g., as a dedicated `constraint_expression` field or child node).\n4. Ensure that field names referenced within the `<constraint-expression>` are captured by the parser for later resolution during type checking and evaluation.",
          "status": "pending",
          "testStrategy": "Unit tests for the parser: \n- Successfully parse `deftype` with valid `:where` clauses and various forms of constraint expressions.\n- Reject `deftype` with syntactically incorrect `:where` clauses.\n- Verify the generated AST structure for type definitions, ensuring the constraint expression is correctly represented and linked."
        },
        {
          "id": 2,
          "title": "Implement Type Checking for Constraint Expressions",
          "description": "Integrate logic into the type checker (Task 4) to verify that constraint expressions provided in `:where` clauses are semantically valid and evaluate to a Boolean type. This ensures type safety for constraints.",
          "dependencies": [
            1
          ],
          "details": "1. Access the constraint AST node from the type definition's AST (created in Subtask 1).\n2. Implement a type checking pass for the constraint expression.\n3. The type checking environment for the constraint expression must include the type's fields (e.g., `field1`, `field2`) with their declared types, making them accessible within the expression.\n4. Verify that the constraint expression ultimately resolves to a Boolean type. If not, report a type error.\n5. Report errors for any undefined variables or type mismatches within the constraint expression itself.",
          "status": "pending",
          "testStrategy": "Unit tests for the type checker component:\n- Constraint expression correctly type-checks to Boolean.\n- Constraint expression type-checks to a non-Boolean type (should error).\n- Constraint expression uses type fields correctly.\n- Constraint expression uses undefined variables or functions (should error).\n- Constraint expression uses fields with incorrect types (should error)."
        },
        {
          "id": 3,
          "title": "Implement Compile-Time Constraint Validation",
          "description": "Enhance the type checker to perform static evaluation of constraints when possible. If a constraint can be proven false at compile-time, the type definition itself should be considered invalid.",
          "dependencies": [
            2
          ],
          "details": "1. After a constraint expression has been successfully type-checked as Boolean (from Subtask 2), attempt to statically evaluate it.\n2. Use the existing evaluation engine (from Task 2 development) for this purpose. The evaluation context should only include compile-time constant values and functions that are safe for compile-time evaluation.\n3. If the constraint expression evaluates to a literal `#f` (false) at compile-time, report a specific compile-time error indicating that the type definition is invalid due to an unsatisfiable constraint.\n4. If the constraint evaluates to a literal `#t` (true), this can be noted (e.g., for potential optimization, though the primary goal is validation).\n5. If the constraint cannot be fully evaluated at compile-time (e.g., it depends on runtime field values or non-foldable function calls), it should be passed through for runtime checking.",
          "status": "pending",
          "testStrategy": "Unit tests for compile-time validation:\n- Type definition with a constraint that is statically true (e.g., `(:where (> 10 5))`).\n- Type definition with a constraint that is statically false (e.g., `(:where (< 0 0))`); verify error reporting.\n- Type definition with a constraint involving literals and simple operations that can be folded.\n- Type definition with a constraint that cannot be evaluated at compile-time (e.g., depends on a field value `(:where (> field1 0))`)."
        },
        {
          "id": 4,
          "title": "Modify CIFs for Runtime Constraint Evaluation",
          "description": "Update the generation and/or evaluation logic of Canonical Instantiation Functions (CIFs) to evaluate type constraints at runtime. If a constraint fails during instantiation, the CIF must raise a specific runtime error.",
          "dependencies": [
            3
          ],
          "details": "1. For types that have a constraint (AST from Subtask 1) not fully resolved or proven true at compile-time (by Subtask 3):\n   a. Modify the CIF logic. After the basic instance of the type is created (fields are populated with initial values), the constraint expression must be evaluated.\n   b. The evaluation environment for the constraint expression within the CIF must bind the type's field names to the corresponding actual values from the instance being created.\n   c. Use the existing evaluation engine (Task 2) for this runtime evaluation.\n2. If the constraint expression evaluates to `#f` (false) at runtime, the CIF must raise a distinct runtime error (e.g., `ConstraintViolationError`). This error should prevent the instantiation of the type and the CIF should not return the partially formed instance.\n3. If the constraint evaluates to `#t` (true), the instantiation process completes normally, and the CIF returns the new instance.",
          "status": "pending",
          "testStrategy": "Integration tests for CIFs:\n- Instantiate a type with a constraint that evaluates to true at runtime using provided field values.\n- Attempt to instantiate a type with a constraint that evaluates to false at runtime; verify that a `ConstraintViolationError` is raised and no instance is returned.\n- Test constraints involving various field values and types.\n- Test constraints that call functions available in the runtime scope."
        },
        {
          "id": 5,
          "title": "Implement Comprehensive Error Reporting for Constraints",
          "description": "Ensure clear, informative, and user-friendly error messages are generated for all potential issues related to type constraints, including syntax errors, type errors, compile-time violations, and runtime violations.",
          "dependencies": [
            4
          ],
          "details": "1. **Syntax Errors (Parser - Subtask 1):** Refine error messages for malformed `:where` clauses (e.g., `SyntaxError: Invalid constraint syntax in :where clause for type 'TypeName'. Expected single expression.`).\n2. **Type Errors (Type Checker - Subtask 2):** Ensure errors clearly state that a constraint expression must be Boolean, or if fields are used incorrectly (e.g., `TypeError: Constraint expression for type 'TypeName' must evaluate to Boolean, got 'Integer'.`).\n3. **Compile-Time Violations (Compile-Time Validation - Subtask 3):** Provide specific messages when a constraint is statically proven false (e.g., `CompileTimeError: Constraint for type 'TypeName' is statically false: '(<constraint-src>)'.`).\n4. **Runtime Violations (CIFs - Subtask 4):** Design the `ConstraintViolationError` to include:\n   - The name of the type being instantiated.\n   - The source form of the constraint expression that failed.\n   - The actual field names and their values for the instance that failed validation (e.g., `RuntimeError: Constraint violation for type 'TypeName'. Constraint '(<constraint-src>)' failed with values (field1: val1, field2: val2).`).\n5. Review and standardize all error messages for consistency and clarity.",
          "status": "pending",
          "testStrategy": "Review existing tests from Subtasks 1-4 to ensure error messages are triggered and are adequate. Create new, specific test cases designed to trigger each category of error, then verify the content, clarity, and helpfulness of the generated messages. For example, test runtime errors with different numbers of fields and value types to ensure proper formatting."
        }
      ]
    },
    {
      "id": 19,
      "title": "Enhanced Error Reporting and Diagnostics System",
      "description": "Implement a comprehensive error reporting and diagnostics system. This system will generate HoTT-aware error messages, report type constraint violations and effect-related errors, and provide user-friendly diagnostic information to improve developer experience.",
      "details": "1. **Error Data Structures**: Define robust data structures for representing various error types (e.g., type mismatch, unbound variable, arity mismatch, constraint violation, effect errors). These structures should capture detailed contextual information: source location (file, line, column), relevant AST nodes, expected vs. actual types/values, and any HoTT-specific information.\n2. **HoTT-Aware Message Generation**: Develop a module to translate error data structures into human-readable, HoTT-aware messages. Messages should use precise HoTT terminology where appropriate (e.g., explaining path equality issues, universe inconsistencies, or type family application errors). Leverage type representations from Task 3 (Basic Type System) for accurate display and explanation of types.\n3. **Type Constraint Violation Reporting**: Integrate with the type checker (Task 4) to receive information about type constraint violations. Clearly explain which constraint was violated and why, providing context from the source code.\n4. **Effect-Related Error Handling Framework**: Design the system to be extensible for effect-related errors. Implement initial handling for any effect-related errors that can be detected with the current system components. Provide clear messages for effect mismatches or unhandled effects.\n5. **User-Friendly Diagnostics**: Focus on clarity, conciseness, and actionability of error messages. Include suggestions for fixes where possible. Ensure error messages clearly point to the source code location of the error.\n6. **Integration**: Integrate this system with the type checker (Task 4) and potentially other compiler phases that can generate errors. Ensure that error reporting is consistent across the system.",
      "testStrategy": "1. **Unit Tests for Error Message Generation**: Create mock error data objects for various scenarios (type mismatches, HoTT-specific errors, constraint violations). Verify that generated messages are accurate, informative, HoTT-aware, and correctly formatted, using type definitions from Task 3.\n2. **Integration Tests with Type Checker (Task 4)**: Use code snippets designed to produce specific type errors. Run them through the type checker (once Task 4 is complete) and verify that the enhanced error reporting system produces the expected user-friendly and HoTT-aware diagnostic messages, including correct source locations.\n3. **Constraint Violation Tests**: Write test cases where type constraints are violated. Ensure error messages clearly identify the violated constraint.\n4. **Effect-Related Error Tests**: Test basic effect error reporting capabilities and the extensibility of the framework using simulated effect errors.\n5. **User-Friendliness Review**: Conduct reviews of error messages for clarity, helpfulness, and actionability.\n6. **HoTT-Awareness Validation**: Specifically test scenarios involving HoTT concepts in errors, verifying correct terminology and explanations by referencing type structures from Task 3.",
      "status": "pending",
      "dependencies": [
        3,
        4
      ],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Core Error Data Structures and Contextual Information Capture",
          "description": "Establish the foundational data structures for representing various error types (e.g., type mismatch, unbound variable, arity mismatch, constraint violation, effect errors). These structures must capture detailed contextual information crucial for diagnostics, including source location, relevant AST nodes, expected vs. actual types/values, and HoTT-specific details.",
          "dependencies": [],
          "details": "1. Define an enumeration or a set of classes/structs for high-level error categories (e.g., `TypeError`, `ScopeError`, `ConstraintViolationError`, `EffectError`).\n2. For each specific error type, define fields to store: source location (file path, line, column, span), references/IDs of relevant AST nodes, expected/actual types or values, and any HoTT-specific information (e.g., path details, universe levels).\n3. Implement a central error accumulator or collector service that compiler phases can use to report instances of these error structures.\n4. Ensure data structures are robust and can be serialized if needed for later analysis or tooling.",
          "status": "pending",
          "testStrategy": "Unit tests: Verify that error data structures can correctly store and retrieve all required contextual information for various simulated error scenarios. Test serialization/deserialization if implemented."
        },
        {
          "id": 2,
          "title": "Develop HoTT-Aware Error Message Generation Engine",
          "description": "Create a module responsible for translating the structured error data (from subtask 1) into human-readable, HoTT-aware error messages. This engine will leverage type representations from Task 3 (Basic Type System).",
          "dependencies": [
            1
          ],
          "details": "1. Implement a primary function or class that accepts an error data object (from subtask 1) as input.\n2. Develop formatting logic for each defined error category and specific error type. Prioritize common errors initially.\n3. Ensure messages utilize precise HoTT terminology where appropriate (e.g., explaining path equality issues, universe inconsistencies, type family application errors).\n4. Integrate with type representation utilities (from external Task 3) to display types accurately and understandably within error messages.\n5. Design the engine to be extensible for new error types and customizable message formats (e.g., plain text, markdown, JSON).",
          "status": "pending",
          "testStrategy": "Unit tests: For the message generation engine, provide various error data objects and verify the output messages for accuracy, clarity, correct use of HoTT terminology, and proper type display. Test with diverse and complex type structures."
        },
        {
          "id": 3,
          "title": "Integrate Type Constraint Violation Reporting with Type Checker",
          "description": "Modify the existing type checker (from external Task 4) to utilize the new error reporting system (subtasks 1 and 2) for reporting type constraint violations. Ensure messages clearly explain the violation and its context.",
          "dependencies": [
            1,
            2
          ],
          "details": "1. Identify all points in the type checker (external Task 4) where type constraints are enforced and violations can be detected.\n2. When a violation is detected, instantiate the appropriate error data structure (from subtask 1) with all relevant contextual information (e.g., the specific constraint, involved types/terms, source location of the constraint and the violation).\n3. Report the populated error object using the central error accumulator/collector.\n4. Verify that the message generation engine (subtask 2) produces clear, informative messages for these constraint violations, explaining which constraint was violated and why, referencing source code.",
          "status": "pending",
          "testStrategy": "Integration tests: Create test code snippets that intentionally violate various type constraints. Run the type checker and verify that the correct error data is generated and the resulting messages (via subtask 2) are accurate, informative, and point to the correct source locations."
        },
        {
          "id": 4,
          "title": "Implement Framework and Initial Handling for Effect-Related Errors",
          "description": "Design the error reporting system to be extensible for effect-related errors and implement initial detection and reporting for such errors, including effect mismatches or unhandled effects.",
          "dependencies": [
            1,
            2
          ],
          "details": "1. Extend the error data structures (subtask 1) to specifically represent effect-related errors (e.g., `EffectMismatchError`, `UnhandledEffectError`, `ForbiddenEffectError`). Include fields for expected effects, actual effects, effect context, and relevant source locations.\n2. Update the message generation engine (subtask 2) to produce clear and specific messages for these new effect error types.\n3. Identify any current system components (e.g., type checker, an early-stage effect analysis pass) that can detect basic effect errors.\n4. Integrate these components to report effect errors using the new structures and message generation.\n5. Ensure the design allows for future expansion as the effect system (if separate) capabilities grow.",
          "status": "pending",
          "testStrategy": "Unit tests: For formatting effect-related error messages. Integration tests: If system components can detect effect errors, write tests with code that triggers these errors and verify the reported errors and messages are correct and helpful."
        },
        {
          "id": 5,
          "title": "Enhance User-Friendliness of Diagnostics and Add Suggestions",
          "description": "Focus on improving the overall developer experience by making all error messages exceptionally clear, concise, actionable, and providing helpful suggestions for fixes where possible. Ensure errors clearly point to source locations.",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "1. Review all error messages generated by the system (from subtask 2, for errors from subtask 3, 4, and others). Refine wording for maximum clarity and conciseness, balancing HoTT precision with understandability.\n2. Implement logic to provide actionable suggestions for common error types (e.g., for unbound variables: \"Did you mean 'variable_x'?\", for type mismatches: \"Consider applying function Y or converting X to Z\").\n3. Ensure all error messages clearly and accurately point to the source code location (file, line, column, span) of the error. Consider mechanisms to display relevant code snippets.\n4. Develop and document guidelines for writing good error messages to maintain quality as new error types are added.\n5. Collect feedback on error messages from potential users if possible.",
          "status": "pending",
          "testStrategy": "Peer review of error messages. Usability testing: Have developers write sample code designed to trigger various errors and evaluate the helpfulness, clarity, and actionability of the diagnostics. A/B testing for different message phrasings if feasible."
        }
      ]
    },
    {
      "id": 20,
      "title": "Implement Basic Proof Search and Constraint Inference System",
      "description": "Develop a system for basic proof search and constraint inference to enable automatic constraint satisfaction. This system will include simple proof generation, type-level computation for constraint checking, and integration with effect polymorphism decisions as specified in the PRD's effect polymorphic CIF system.",
      "details": "**1. Proof Representation and Basic Search Algorithm:**\n   - Define data structures for representing proof steps, proof trees, and logical formulae relevant to constraints.\n   - Implement a foundational proof search algorithm (e.g., a simple tableau prover, or a resolution-based method tailored to the constraint language from Task 18).\n   - The search should be capable of taking a constraint goal and attempting to derive its validity based on axioms and existing type information.\n\n**2. Constraint Inference Engine:**\n   - Design and implement a set of inference rules applicable to the language's constraint expressions (e.g., transitivity, symmetry for equality, rules for type properties, custom rules from PRD).\n   - Create an engine that can apply these rules to a given set of constraints to deduce new, implied constraints. This engine will support the proof search mechanism.\n\n**3. Type-Level Computation for Constraint Checking:**\n   - Extend the constraint validation logic (from Task 18) to utilize the proof search and inference engine.\n   - During type checking or CIF instantiation, when a constraint needs verification, the system will first attempt direct evaluation. If this is insufficient, it will invoke the proof search mechanism.\n   - Enable the evaluation or symbolic reasoning about type-level functions or predicates within constraints.\n\n**4. Simple Proof Generation:**\n   - Upon successful satisfaction of a constraint via proof search, the system should be able to generate a simplified trace or certificate of the proof.\n   - This output can be used for debugging, providing explanations to the developer, or potentially for more advanced compiler analyses in the future.\n\n**5. Integration with Effect Polymorphism (as per PRD):**\n   - Thoroughly review the PRD specifications for the \"effect polymorphic CIF system.\"\n   - Ensure the proof search and constraint inference mechanisms are equipped to handle constraints related to computational effects (e.g., purity, specific effect signatures, effect subsumption).\n   - The system must aid in deciding or verifying effect-related properties for polymorphic types and functions, aligning with the PRD's requirements for CIFs. For instance, inferring that a function `f : (A -> B) -> C` is pure if its argument function `(A -> B)` is proven pure via constraints.",
      "testStrategy": "**1. Unit Tests:**\n   - **Proof Search:** Create test cases with constraints that are trivially true, require multi-step derivations, and are unsatisfiable. Verify correct outcomes and termination.\n   - **Constraint Inference:** Test individual inference rules with known inputs and expected outputs. Verify the engine's ability to derive a complete set of implied constraints.\n   - **Type-Level Computation:** Define types with constraints involving type-level functions/predicates (e.g., list length, arithmetic properties) and verify correct validation by the proof system.\n   - **Proof Generation:** For constraints proven true, inspect the generated proof trace for correctness and clarity.\n\n**2. Integration Tests:**\n   - **Interaction with Task 18 (Constraint System):**\n     - Define `deftype` expressions with `:where` clauses that necessitate the proof search/inference capabilities (e.g., involving quantifiers or complex logical combinations if supported).\n     - Test that types are correctly accepted/rejected based on the provability of their constraints during compile-time checks.\n     - Test CIF instantiation where runtime constraint checks leverage the inference engine.\n   - **Effect Polymorphism Scenarios (aligned with PRD):**\n     - Implement test cases for polymorphic functions and types with effect-related constraints (e.g., a higher-order function whose effect signature depends on the proven effects of its function arguments).\n     - Verify that the system correctly infers or proves these effect properties, leading to correct type checking and CIF behavior as per the PRD.\n     - Example: Test a scenario where a CIF's effect polymorphism is resolved by proving a constraint like `(effect_of(X) subset_of allowed_effects)`.\n\n**3. Scenario-Based Testing (PRD Alignment):**\n   - Develop comprehensive test scenarios based directly on examples or specifications from the PRD for the effect polymorphic CIF system.\n   - Ensure the end-to-end behavior (parsing, type checking, constraint satisfaction, CIF instantiation) matches the PRD's intent when the new proof search and inference system is active.",
      "status": "pending",
      "dependencies": [
        18
      ],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Define Proof Data Structures and Implement Foundational Proof Search",
          "description": "Establish core data structures for representing proof steps, proof trees, and logical formulae for constraints. Implement a foundational proof search algorithm (e.g., tableau or resolution-based) capable of deriving constraint validity from axioms and existing type information.",
          "dependencies": [],
          "details": "Data structures should be compatible with constraints defined in Task 18. The search algorithm will take a constraint goal and attempt to prove its validity. Initial implementation can use a simple depth-first or breadth-first search. Axioms and type information form the knowledge base.",
          "status": "pending",
          "testStrategy": "Unit test data structure integrity and serialization/deserialization. Test the search algorithm with basic constraint goals (provable and unprovable) against a minimal set of predefined axioms."
        },
        {
          "id": 2,
          "title": "Develop Constraint Inference Engine with Rule Application Logic",
          "description": "Design and implement a set of inference rules (e.g., transitivity, symmetry, type property rules, custom rules from PRD). Create an engine that applies these rules to a given set of constraints to deduce new, implied constraints, thereby supporting the proof search mechanism.",
          "dependencies": [
            1
          ],
          "details": "The inference engine will take known constraints and apply rules to expand this set. Rules should be designed for extensibility and configurability. This engine will be invoked by the proof search (from subtask 1) to generate intermediate proof steps or explore new deduction paths.",
          "status": "pending",
          "testStrategy": "Unit test individual inference rules for correctness. Test the engine's ability to derive expected implied constraints from a given set using various rule combinations. Verify correct interaction with the proof search mechanism from subtask 1."
        },
        {
          "id": 3,
          "title": "Integrate Proof System into Type-Level Constraint Checking",
          "description": "Extend the existing constraint validation logic (from Task 18) to utilize the proof search and inference engine. When direct evaluation of a constraint is insufficient, the system will invoke the proof mechanism. Enable symbolic reasoning about type-level functions or predicates within constraints.",
          "dependencies": [
            1,
            2
          ],
          "details": "Modify the type checking or Constraint Intermediate Form (CIF) instantiation logic. If a constraint check fails direct evaluation, formulate it as a goal for the proof search system. The system must handle type-level functions/predicates, potentially by treating them as uninterpreted functions or by applying specific axioms/rules defined for them.",
          "status": "pending",
          "testStrategy": "Test scenarios where constraints are solved by direct evaluation versus those requiring the proof search and inference engine. Verify correct handling and reasoning for type-level functions/predicates within constraints, ensuring the proof system is invoked appropriately."
        },
        {
          "id": 4,
          "title": "Implement Simple Proof Trace Generation for Successful Proofs",
          "description": "Upon successful satisfaction of a constraint via the proof search mechanism, the system should generate a simplified trace or certificate of the proof. This output will serve debugging, developer explanation, or future compiler analyses.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Augment the proof search algorithm (from subtask 1) and inference engine (subtask 2) to record the sequence of applied rules, axioms, and derived facts leading to the proof. Develop a formatter to present this trace in a human-readable or structured (e.g., JSON) format. Focus on clarity and essential information for debugging and understanding the proof.",
          "status": "pending",
          "testStrategy": "For various constraints successfully proven by the integrated system (subtask 3), verify that the generated proof trace accurately and clearly reflects the deduction steps. Check the output format, completeness, and readability of the trace."
        },
        {
          "id": 5,
          "title": "Adapt Proof System for Effect Polymorphism Constraints (PRD)",
          "description": "Review PRD specifications for the 'effect polymorphic CIF system.' Adapt the proof search and constraint inference mechanisms to handle constraints related to computational effects (e.g., purity, specific effect signatures, effect subsumption). The system must aid in deciding or verifying effect-related properties for polymorphic types and functions.",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Identify specific effect-related constraint types from the PRD (e.g., `isPure(X)`, `hasEffect(E, X)`). Define corresponding axioms or inference rules (e.g., for purity propagation, effect composition, effect subsumption). Integrate these into the inference engine (subtask 2) and ensure the proof search (subtask 1) can utilize them within the context of constraint checking (subtask 3). Example: inferring purity of `f : (A -> B) -> C` if its argument function `(A -> B)` is proven pure via constraints.",
          "status": "pending",
          "testStrategy": "Develop test cases based on PRD examples for effect polymorphism. Verify the system's ability to correctly infer or validate effect-related properties, such as purity of higher-order functions or effect subsumption between types, using the implemented proof and inference capabilities."
        }
      ]
    },
    {
      "id": 21,
      "title": "Set Up Racket/Rhombus Development Environment and Project Structure",
      "description": "Establish the development environment using Racket #lang (or Rhombus with shrubbery/forest macros), including implementation language choice, initial project structure, build system, testing framework, and development tools. This task is foundational for all subsequent implementation work.",
      "details": "1. **Language Choice & Setup**: Evaluate and decide between standard Racket (e.g., `#lang racket/base`) and Rhombus. If Rhombus, identify and integrate necessary base layers like `shrubbery` and `forest`. Document the choice and install the chosen Racket version or Rhombus distribution.\n2. **Project Structure**: Create a standard directory layout: `src/` for core source code, `tests/` for unit/integration tests, `examples/` for sample programs, `docs/` for documentation, and `scripts/` for utility scripts. Initialize a main entry point file if applicable (e.g., `src/main.rkt`).\n3. **Version Control**: Initialize a Git repository (`git init`). Create a comprehensive `.gitignore` file tailored for Racket/Rhombus projects (e.g., ignoring `compiled/` directories, editor-specific files, OS-specific files).\n4. **Build System**: Configure Racket's `raco make` for building the project or set up the equivalent build process for Rhombus. Create initial build configuration files (e.g., `info.rkt` for Racket package metadata if the project will be structured as a package).\n5. **Testing Framework**: Integrate the `rackunit` testing framework. Create a main test runner file (e.g., `tests/main-tests.rkt`) to discover and run all tests. Include a simple placeholder test.\n6. **Dependency Management**: Document the process for managing external Racket packages using `raco pkg`. List any known core external libraries required from the outset.\n7. **Development Tools**: Recommend and document setup for IDEs/editors (e.g., DrRacket, VS Code with Racket LSP, Emacs with racket-mode). Investigate and optionally set up linters (e.g., `raco lint`) and code formatters.\n8. **Documentation Setup**: Set up Scribble for project documentation. Create a basic `docs/main.scrbl` file and configure `raco setup` or `raco scribble` for building documentation.",
      "testStrategy": "1. **Environment Verification**: Confirm successful installation and activation of the chosen Racket version or Rhombus environment on a clean system or in a container.\n2. **Project Initialization & Git**: Clone the repository after the initial commit. Verify the directory structure matches the definition. Confirm the `.gitignore` file is present and correctly excludes common Racket artifacts (e.g., create dummy compiled files, check `git status`).\n3. **Build Process**: Create a minimal 'hello world' module in `src/`. Successfully build the project using the configured build command (e.g., `raco make src/main.rkt`). Verify compiled outputs are generated and handled correctly by version control.\n4. **Testing Framework**: Create a simple test case in `tests/` (e.g., using `rackunit` `check-equal?`). Run the test suite (e.g., `raco test tests/main-tests.rkt`) and confirm tests are discovered and pass.\n5. **REPL & Tooling**: Launch the Racket/Rhombus REPL in the project directory. Successfully `require` a module from `src/`. If linters/formatters are set up, run them on a sample file and verify they function as expected.\n6. **Dependency Management Check**: If a sample external dependency is identified, attempt to install it via `raco pkg install` and `require` it in a test module to ensure the process works.\n7. **Documentation Generation**: Add minimal content to `docs/main.scrbl`. Run the documentation build command (e.g., `raco scribble +m docs/main.scrbl`) and verify that HTML or other specified output is generated correctly.",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Language Selection, Installation, and Version Control Initialization",
          "description": "Decide between standard Racket and Rhombus, install the chosen language environment, initialize a Git repository, and create a tailored .gitignore file.",
          "dependencies": [],
          "details": "1. **Language Evaluation & Choice**: Evaluate standard Racket (e.g., `#lang racket/base`) versus Rhombus (potentially with `shrubbery` and `forest` macros). Document the decision and its rationale.\n2. **Installation**: Install the chosen Racket version or Rhombus distribution. Verify the installation by running the REPL (e.g., `racket` or `rhombus`).\n3. **Version Control Setup**: Initialize a Git repository in the project's root directory using `git init`.\n4. **.gitignore File**: Create a comprehensive `.gitignore` file. Include patterns for Racket/Rhombus compiled files (e.g., `compiled/`, `*.zo`), editor-specific files (e.g., `.vscode/`, `*.elc`), OS-specific files (e.g., `.DS_Store`, `Thumbs.db`), and any local configuration files.",
          "status": "done",
          "testStrategy": "Verify that the chosen Racket/Rhombus REPL starts correctly. Confirm that `git status` shows a clean working directory after an initial commit that includes the `.gitignore` file."
        },
        {
          "id": 2,
          "title": "Establish Core Project Directory Structure and Basic Build Configuration",
          "description": "Create the standard directory layout for source code, tests, examples, documentation, and scripts. Set up the initial build system configuration using Racket's tools.",
          "dependencies": [
            1
          ],
          "details": "1. **Directory Structure**: Create the following directories in the project root: `src/` (for core source code), `tests/` (for unit and integration tests), `examples/` (for sample programs using the library/application), `docs/` (for project documentation), and `scripts/` (for utility and build scripts).\n2. **Main Entry Point**: Create an initial main entry point file if applicable (e.g., `src/main.rkt` containing a basic module definition like `(#lang racket/base (provide main)) (define (main) (displayln \"Hello, Project!\"))`).\n3. **Build System Setup**: Configure Racket's `raco make` for building the project. This involves creating an `info.rkt` file in the project root if structuring the project as a Racket package. The `info.rkt` should define essential metadata like project name, version, and initial (empty or core) dependencies.\n4. **Initial Build Test**: Ensure that `raco make` (or the Rhombus equivalent) can process the `info.rkt` and compile files in `src/`.",
          "status": "done",
          "testStrategy": "Verify that all specified directories exist. Execute `raco make` (or equivalent build command); it should complete without errors, potentially compiling `src/main.rkt`."
        },
        {
          "id": 3,
          "title": "Integrate Testing Framework and Create Initial Test Suite",
          "description": "Integrate the `rackunit` testing framework. Create a main test runner file and include a simple placeholder test to ensure the testing setup is functional.",
          "dependencies": [
            2
          ],
          "details": "1. **Framework Integration**: Ensure `rackunit` is available (it's typically bundled with Racket). If developing a package, list `rackunit` as a test dependency in `info.rkt`.\n2. **Test Runner**: Create a main test runner file, e.g., `tests/main-tests.rkt`. This file should `(require rackunit)` and be structured to discover and run all test files within the `tests/` directory (e.g., by explicitly requiring other test files or using a helper to find them).\n3. **Placeholder Test**: Add a simple placeholder test case in `tests/main-tests.rkt` or a separate `tests/example-test.rkt` file (e.g., `(require rackunit) (test-case \"Placeholder Test\" (check-true #t \"This is a basic truth test.\"))`).\n4. **Test Execution Script/Command**: Configure the build system or create a script (e.g., in `scripts/run-tests.sh` or a Racket script `scripts/run-tests.rkt`) to execute all tests (e.g., by running `racket tests/main-tests.rkt`).",
          "status": "done",
          "testStrategy": "Execute the test runner script/command. The output should indicate that the test suite ran and the placeholder test passed."
        },
        {
          "id": 4,
          "title": "Set Up Dependency Management and Initial Documentation Infrastructure",
          "description": "Document the process for managing external Racket packages using `raco pkg`. Set up the basic infrastructure for project documentation using Scribble, including an initial document and build configuration.",
          "dependencies": [
            2
          ],
          "details": "1. **Dependency Management Documentation**: In a `README.md` or `CONTRIBUTING.md` file, document how to manage external Racket packages using `raco pkg` commands (e.g., `raco pkg install <package>`, `raco pkg update <package>`).\n2. **Initial Dependencies**: List any known core external libraries required from the outset. If the project is a package, add these to the `deps` list in `info.rkt`.\n3. **Scribble Setup**: Set up Scribble for project documentation. Create a basic `docs/main.scrbl` file with a title (e.g., `@title[My Project Documentation]`) and a minimal section.\n4. **Documentation Build**: Configure `raco setup` (if the project is a package and `info.rkt` specifies Scribble docs) or use `raco scribble` commands (e.g., `raco scribble --htmls ++xref-in Scribble-xref.rktd docs/main.scrbl`) to build the documentation. Consider adding a script to `scripts/build-docs.sh` for this process.",
          "status": "done",
          "testStrategy": "Attempt to install a known, simple Racket package using `raco pkg install <some-package>` to verify the documented process. Build the initial Scribble documentation and check that the HTML output is generated in the expected location (e.g., `docs/html/index.html`)."
        },
        {
          "id": 5,
          "title": "Configure Development Tools and Finalize Project Scripts",
          "description": "Recommend and document setup for IDEs/editors, linters (e.g., `raco lint`), and code formatters. Review and finalize utility scripts for common development tasks.",
          "dependencies": [
            4
          ],
          "details": "1. **IDE/Editor Setup**: Document recommended IDEs/editors (e.g., DrRacket, VS Code with Racket/LSP extension, Emacs with racket-mode). Provide basic setup instructions or links to relevant extensions/plugins in `README.md` or a development guide.\n2. **Linter Setup**: Investigate and set up `raco lint` for static analysis. Document its usage (e.g., `raco lint src/**/*.rkt tests/**/*.rkt`). Consider integrating it into a pre-commit hook or a CI pipeline script.\n3. **Code Formatter (Optional)**: Investigate and optionally set up a code formatter compatible with Racket/Rhombus. Document its usage if adopted.\n4. **Finalize Utility Scripts**: Review, refine, and document all scripts in the `scripts/` directory (e.g., for building the project, running tests, generating documentation, linting code). Ensure they are executable and robust.",
          "status": "done",
          "testStrategy": "Verify that the recommended IDE/editor extensions can be installed and provide basic functionality (e.g., syntax highlighting for `.rkt` files). Run `raco lint` on the existing codebase and ensure it executes. Execute all scripts in the `scripts/` directory to confirm they work as intended and produce the expected outcomes."
        }
      ]
    }
  ]
}